âœ… ë³´ì™„í•  ì  5ê°€ì§€
1. Feature/Label ê°„ ë§¤í•‘ ë¡œê·¸ ëˆ„ë½
ë¬¸ì œì : Xì™€ y ê°„ì˜ EPC ë§¤í•‘ ì •ë³´ê°€ ë¹ ì ¸ ìˆìŠµë‹ˆë‹¤. ë””ë²„ê¹…ì´ë‚˜ ì˜¤íƒ ë¶„ì„í•  ë•Œ ì–´ë–¤ EPCê°€ ì´ìƒì¹˜ë¡œ ë¶„ë¥˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.

ê°œì„ ì•ˆ:

data_manager.pyì— epc_list.json ë˜ëŠ” epc_list.npyë¥¼ anomaly_typeë³„ë¡œ ì €ì¥í•˜ì„¸ìš”.

ì˜ˆ:

python
ë³µì‚¬
í¸ì§‘
epc_list[anomaly_type] = [epc_code1, epc_code2, ...]
2. ë¼ë²¨ë§ ì‹ ë¢°ë„ ê¸°ë¡ ë¯¸í¬í•¨
ë¬¸ì œì : Rule-based ë¼ë²¨ì´ "ì™„ë²½í•œ ground truth"ê°€ ì•„ë‹˜ì—ë„ ë¶ˆêµ¬í•˜ê³  ì´ì§„ê°’ìœ¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤.

ê°œì„ ì•ˆ:

rule_based_labels.pyì—ì„œ ë¼ë²¨ê³¼ í•¨ê»˜ ì›ë˜ ì ìˆ˜(score) ë„ í•¨ê»˜ ë¦¬í„´í•˜ì„¸ìš”.

python
ë³µì‚¬
í¸ì§‘
return {'epcFake': (label, score), ...}
ì´ë¥¼ í†µí•´ borderline ì¼€ì´ìŠ¤(ì˜ˆ: 49ì  vs 51ì ) êµ¬ë¶„ ê°€ëŠ¥.

3. í…ŒìŠ¤íŠ¸ì…‹ ë¶„ë¦¬ ì „ëµ ë¯¸ì •
ë¬¸ì œì : ì „ì²˜ë¦¬ í›„ ë°ì´í„°ë¥¼ SVM í•™ìŠµìš©ìœ¼ë¡œ ì €ì¥í•˜ì§€ë§Œ, train/test ë¶„ë¦¬ ì „ëµì´ ë¬¸ì„œì— ì—†ìŠµë‹ˆë‹¤.

ê°œì„ ì•ˆ:

pipeline.py ë˜ëŠ” data_manager.pyì—ì„œ ìë™ìœ¼ë¡œ train/test split ê¸°ëŠ¥ ì¶”ê°€ ì œì•ˆ.

python
ë³µì‚¬
í¸ì§‘
sklearn.model_selection.train_test_split(X, y, test_size=0.2)
4. ì‹¤ì‹œê°„ ì ìš© ëŒ€ë¹„ í”¼ì²˜ ê³„ì‚° ë¹„ìš© ê³ ë ¤ ë¶€ì¡±
ë¬¸ì œì : ì¼ë¶€ feature í•¨ìˆ˜ (ì˜ˆ: _calculate_distance_features, _calculate_entropy, _extract_time_intervals) ëŠ” ì—°ì‚°ëŸ‰ì´ í½ë‹ˆë‹¤.

ê°œì„ ì•ˆ:

config.py ì— runtime_safe = True/False í”Œë˜ê·¸ ë„ì…í•´ì„œ, ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤ìš© feature ì¡°í•©ê³¼ ë°°ì¹˜ìš© feature ì¡°í•©ì„ êµ¬ë¶„í•˜ì„¸ìš”.

ì˜ˆ:

python
ë³µì‚¬
í¸ì§‘
if config['runtime_safe']:
    exclude_entropy = True
5. ë‹¨ì¼ entrypoint CLI ë˜ëŠ” test harness ì—†ìŒ
ë¬¸ì œì : ì§€ê¸ˆì€ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì´ ê°ì²´ ê¸°ë°˜ í˜¸ì¶œë§Œ ìˆê³ , CLI/í…ŒìŠ¤íŠ¸ ì§„ì…ì ì´ ì—†ì–´ ê°œë°œ ì¤‘ í™•ì¸ì´ ë¶ˆí¸í•©ë‹ˆë‹¤.

ê°œì„ ì•ˆ:

scripts/run_pipeline.py ë˜ëŠ” main.py ë¥¼ ì¶”ê°€í•´ CLI ì§„ì…ì  ì œê³µ:

bash
ë³µì‚¬
í¸ì§‘
python scripts/run_pipeline.py --input=data/raw.csv --output=data/svm_training
ë˜ëŠ” pytest ê¸°ë°˜ í…ŒìŠ¤íŠ¸ harness ì¶”ê°€.



âœ… 1. EPC ë§¤í•‘ ëˆ„ë½ â†’ "ëˆ„ê°€ ë¬¸ì œì˜€ëŠ”ì§€ ëª¨ë¥´ë©´ í•´ì„ì´ ë¶ˆê°€ëŠ¥í•¨"
SVMì´ 1ì´ë¼ê³  ì˜ˆì¸¡í–ˆëŠ”ë° ì–´ë–¤ EPCì˜€ëŠ”ì§€ ëª¨ë¥´ë©´ ì™œ ê·¸ëŸ° ê²°ê³¼ê°€ ë‚˜ì™”ëŠ”ì§€ ë¶„ì„ì´ ì•ˆ ë¨.

â†’ ë°˜ë“œì‹œ X[i], y[i], epc_code[i]ê°€ ë§¤ì¹­ë˜ì–´ì•¼ ë””ë²„ê¹…/í•´ì„ ê°€ëŠ¥.

âœ… 2. ë¼ë²¨ ì ìˆ˜ ì •ë³´ ì—†ìŒ â†’ "ì‹ ë¢°ë„ ì—†ëŠ” ì´ì§„ ë¶„ë¥˜ëŠ” ìœ„í—˜í•¨"
ì˜ˆ: ì ìˆ˜ 49 vs 51 â†’ ê±°ì˜ ê°™ì€ë° 0, 1ë¡œ ì™„ì „íˆ ê°ˆë¦¼.

â†’ scoreë„ ê°™ì´ ì €ì¥í•´ë‘ë©´ ë‚˜ì¤‘ì— ëª¨ë¸ì´ í—·ê°ˆë¦¬ëŠ” ì‚¬ë¡€ë¥¼ í™•ì¸í•˜ê±°ë‚˜ í›„ì²˜ë¦¬ ê°€ëŠ¥.

âœ… 3. í…ŒìŠ¤íŠ¸ì…‹ ë¶„ë¦¬ ì—†ìŒ â†’ "í•™ìŠµ ì˜ ëëŠ”ì§€ í‰ê°€í•  ë°©ë²• ì—†ìŒ"
ì „ì²˜ë¦¬ë§Œ í•˜ê³  ëª¨ë¸ í•™ìŠµì— ë°”ë¡œ ë‹¤ ë„£ìœ¼ë©´ ê³¼ì í•©ì¸ì§€ ëª¨ë¦„.

â†’ ê¸°ë³¸ì ìœ¼ë¡œ train_test_split() í•´ì„œ ë¶„ë¦¬í•´ë‘ë©´ ë°”ë¡œ ì‹¤í—˜ ê°€ëŠ¥.

âœ… 4. ì—°ì‚°ë¹„ìš© ê³ ë ¤ ì—†ìŒ â†’ "ì‹¤ì‹œê°„ ì‹œìŠ¤í…œì—ì„œ ì£½ì„ ìˆ˜ ìˆìŒ"
ì¼ë¶€ featureëŠ” ë„ˆë¬´ ë¬´ê±°ì›Œì„œ ì‹¤ì‹œê°„ APIì— ì“°ê¸°ì—” ë¶€ë‹´ í¼.

â†’ configì— runtime_safe ì˜µì…˜ ì£¼ë©´, ê°€ë³ê³  ë¹ ë¥¸ featureë§Œ ê³¨ë¼ì„œ ì ìš© ê°€ëŠ¥.

âœ… 5. ì§„ì…ì  ì—†ìŒ â†’ "ì‹¤ì œë¡œ ëŒë ¤ë³´ê¸° ë„ˆë¬´ ë¶ˆí¸í•¨"
ë§¤ë²ˆ pipeline ì¸ìŠ¤í„´ìŠ¤ ë§Œë“¤ì–´ì„œ ë¶ˆëŸ¬ì˜¤ëŠ” ê±´ ê·€ì°®ê³  ì‹¤ìˆ˜ ë§ìŒ.

â†’ run_pipeline.py í•˜ë‚˜ ìˆìœ¼ë©´ python run_pipeline.py --input ...ë§Œìœ¼ë¡œ ë¹ ë¥´ê²Œ ëŒë¦´ ìˆ˜ ìˆìŒ.

SVM ì „ì²˜ë¦¬ ê³„íš - 5ê°€ì§€ í•µì‹¬ ìˆ˜ì •ì‚¬í•­
ğŸš¨ ìˆ˜ì •ì´ í•„ìš”í•œ 5ê°€ì§€ ë¬¸ì œì 
1. Feature ì°¨ì› ë¶ˆì¼ì¹˜ ë¬¸ì œ ğŸ”§
âŒ í˜„ì¬ ë¬¸ì œ:

python
# ê° Feature Extractorê°€ ë‹¤ë¥¸ ê¸¸ì´ì˜ ë²¡í„° ë°˜í™˜
epc_features = [1.0, 0.0, 1.0, 0.5, ...]     # ê¸¸ì´ 8
dup_features = [2.0, 1.0, 3.0, 1.0]          # ê¸¸ì´ 4
order_features = [3.0, 2.0, 1.0, 0.8, 0.6]  # ê¸¸ì´ 5
âœ… í•´ê²°ì±…:

python
class FeatureExtractor:
    """ëª¨ë“  extractorëŠ” ê³ ì •ëœ ì°¨ì›ì„ ë°˜í™˜í•´ì•¼ í•¨"""
    
    FEATURE_DIMENSIONS = {
        'epcFake': 10,      # ê³ ì • 10ì°¨ì›
        'epcDup': 8,        # ê³ ì • 8ì°¨ì›  
        'evtOrderErr': 12,  # ê³ ì • 12ì°¨ì›
        'locErr': 15,       # ê³ ì • 15ì°¨ì›
        'jump': 10          # ê³ ì • 10ì°¨ì›
    }
    
    def extract_features(self, data) -> List[float]:
        features = self._calculate_features(data)
        # íŒ¨ë”© ë˜ëŠ” íŠ¸ë ì¼€ì´ì…˜ìœ¼ë¡œ ê³ ì • ê¸¸ì´ ë³´ì¥
        return self._normalize_to_fixed_length(features, self.expected_dim)
2. í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ ëˆ„ë½ âš–ï¸
âŒ í˜„ì¬ ë¬¸ì œ:

python
# ì‹¤ì œ ë°ì´í„°ì—ì„œ ì˜ˆìƒë˜ëŠ” ë¶„í¬
epcFake:    ì •ìƒ 95% vs ì´ìƒ 5%    # ì‹¬ê°í•œ ë¶ˆê· í˜•
epcDup:     ì •ìƒ 98% vs ì´ìƒ 2%    # ë” ì‹¬ê°í•œ ë¶ˆê· í˜•
evtOrderErr: ì •ìƒ 90% vs ì´ìƒ 10%
âœ… í•´ê²°ì±…:

python
class ImbalanceHandler:
    """í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°ì„ ìœ„í•œ ì „ëµë“¤"""
    
    def handle_imbalance(self, X: np.ndarray, y: np.ndarray, strategy: str = 'smote'):
        if strategy == 'smote':
            from imblearn.over_sampling import SMOTE
            smote = SMOTE(random_state=42)
            return smote.fit_resample(X, y)
        
        elif strategy == 'weighted':
            # class_weight='balanced' ì‚¬ìš©ì„ ìœ„í•œ ë©”íƒ€ë°ì´í„° ë°˜í™˜
            pos_weight = len(y) / (2 * np.sum(y))
            neg_weight = len(y) / (2 * (len(y) - np.sum(y)))
            return X, y, {0: neg_weight, 1: pos_weight}
        
        elif strategy == 'threshold_tuning':
            # ì„ê³„ê°’ ìµœì í™”ë¥¼ ìœ„í•œ ë‹¤ì–‘í•œ threshold ë°˜í™˜
            return X, y, [30, 40, 50, 60, 70]  # ë‹¤ì–‘í•œ ì„ê³„ê°’ í…ŒìŠ¤íŠ¸

# config.pyì— ì¶”ê°€
SVM_CONFIG = {
    'class_balance': {
        'strategy': 'smote',  # 'smote', 'weighted', 'threshold_tuning'
        'min_samples_per_class': 50,  # ìµœì†Œ ìƒ˜í”Œ ìˆ˜
        'fallback_strategy': 'weighted'  # SMOTE ì‹¤íŒ¨ì‹œ ëŒ€ì•ˆ
    }
}
3. Feature ì •ê·œí™”/ìŠ¤ì¼€ì¼ë§ ëˆ„ë½ ğŸ“
âŒ í˜„ì¬ ë¬¸ì œ:

python
# SVMì€ feature scaleì— ë§¤ìš° ë¯¼ê°í•¨
features = [
    1.0,        # boolean (0-1 ë²”ìœ„)
    156.8,      # EPC ê¸¸ì´ (0-200 ë²”ìœ„)  
    0.000001,   # entropy (0-1 ë²”ìœ„)
    23847.5     # time_diff_hours (0-ìˆ˜ë§Œ ë²”ìœ„)
]
# â†’ SVMì´ í° ê°’ì—ë§Œ ì˜ì¡´í•˜ê²Œ ë¨
âœ… í•´ê²°ì±…:

python
from sklearn.preprocessing import StandardScaler, RobustScaler

class FeatureNormalizer:
    """SVMìš© feature ì •ê·œí™”"""
    
    def __init__(self, method: str = 'robust'):
        self.method = method
        self.scalers = {}
    
    def fit_transform_features(self, X: np.ndarray, anomaly_type: str) -> np.ndarray:
        if self.method == 'robust':
            # ì´ìƒì¹˜ì— ëœ ë¯¼ê°í•œ RobustScaler ì‚¬ìš©
            scaler = RobustScaler()
        else:
            scaler = StandardScaler()
        
        X_scaled = scaler.fit_transform(X)
        self.scalers[anomaly_type] = scaler  # ì˜ˆì¸¡ì‹œ ì¬ì‚¬ìš©ìœ„í•´ ì €ì¥
        return X_scaled
    
    def transform_features(self, X: np.ndarray, anomaly_type: str) -> np.ndarray:
        """ì˜ˆì¸¡ì‹œ ì‚¬ìš© (ì´ë¯¸ fitëœ scaler ì‚¬ìš©)"""
        return self.scalers[anomaly_type].transform(X)

# data_manager.pyì— í†µí•©
def save_training_data(self, X, y, anomaly_type):
    # ì •ê·œí™” ì ìš©
    normalizer = FeatureNormalizer()
    X_scaled = normalizer.fit_transform_features(X, anomaly_type)
    
    # scalerë„ í•¨ê»˜ ì €ì¥ (ì˜ˆì¸¡ì‹œ í•„ìš”)
    scaler_path = f"{self.output_dir}/{anomaly_type}_scaler.joblib"
    joblib.dump(normalizer.scalers[anomaly_type], scaler_path)
    
    np.save(f"{self.output_dir}/{anomaly_type}_X_train.npy", X_scaled)
4. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ë¬¸ì œ ğŸ§ 
âŒ í˜„ì¬ ë¬¸ì œ:

python
# ëª¨ë“  ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— í•œë²ˆì— ë¡œë“œ
for epc_code, epc_group in df_clean.groupby('epc_code'):  # ìˆ˜ë°±ë§Œ EPC
    all_features['epcFake'].append(extract_features(...))  # ë©”ëª¨ë¦¬ í­ì¦
    all_features['epcDup'].append(extract_features(...))
    # ... 5ê°œ anomaly type Ã— ìˆ˜ë°±ë§Œ EPC = OOM
âœ… í•´ê²°ì±…:

python
class BatchProcessor:
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬"""
    
    def __init__(self, batch_size: int = 10000):
        self.batch_size = batch_size
    
    def process_in_batches(self, df: pd.DataFrame) -> Generator:
        """EPCë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬"""
        epc_codes = df['epc_code'].unique()
        
        for i in range(0, len(epc_codes), self.batch_size):
            batch_epcs = epc_codes[i:i + self.batch_size]
            batch_df = df[df['epc_code'].isin(batch_epcs)]
            yield batch_df
    
    def save_batch_features(self, features_batch: Dict, batch_idx: int):
        """ë°°ì¹˜ë³„ë¡œ ì„ì‹œ ì €ì¥"""
        for anomaly_type, features in features_batch.items():
            temp_path = f"temp_{anomaly_type}_batch_{batch_idx}.npy"
            np.save(temp_path, np.array(features))
    
    def merge_batches(self, anomaly_type: str, total_batches: int):
        """ë°°ì¹˜ë“¤ì„ ìµœì¢… ë³‘í•©"""
        all_features = []
        for i in range(total_batches):
            batch_features = np.load(f"temp_{anomaly_type}_batch_{i}.npy")
            all_features.append(batch_features)
            os.remove(f"temp_{anomaly_type}_batch_{i}.npy")  # ì •ë¦¬
        
        return np.vstack(all_features)

# pipeline.py ìˆ˜ì •
def process_data(self, raw_df: pd.DataFrame):
    batch_processor = BatchProcessor(batch_size=10000)
    
    for batch_idx, batch_df in enumerate(batch_processor.process_in_batches(raw_df)):
        batch_features = self._process_batch(batch_df)
        batch_processor.save_batch_features(batch_features, batch_idx)
    
    # ìµœì¢… ë³‘í•©
    for anomaly_type in self.extractors.keys():
        final_features = batch_processor.merge_batches(anomaly_type, batch_idx + 1)
        self.data_manager.save_training_data(final_features, anomaly_type)
5. ì˜ˆì¸¡ ë‹¨ê³„ ì—°ë™ ëˆ„ë½ ğŸ”®
âŒ í˜„ì¬ ë¬¸ì œ:

ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ë§Œ ìˆê³ , ì‹¤ì œ SVM ì˜ˆì¸¡ì—ì„œ ì–´ë–»ê²Œ ì‚¬ìš©í• ì§€ ë¶ˆëª…í™•
ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë“¤ì–´ì™”ì„ ë•Œ ì „ì²˜ë¦¬ ê³¼ì • ì¬í˜„ ë¶ˆê°€
âœ… í•´ê²°ì±…:

python
class SVMInferenceEngine:
    """í›ˆë ¨ëœ SVM ëª¨ë¸ì„ ì´ìš©í•œ ì‹¤ì‹œê°„ ì˜ˆì¸¡"""
    
    def __init__(self, model_dir: str):
        self.model_dir = model_dir
        self.models = {}
        self.scalers = {}
        self.extractors = {}
        self._load_trained_models()
    
    def _load_trained_models(self):
        """ì €ì¥ëœ ëª¨ë¸ë“¤ ë¡œë“œ"""
        for anomaly_type in ['epcFake', 'epcDup', 'evtOrderErr', 'locErr', 'jump']:
            # SVM ëª¨ë¸ ë¡œë“œ
            model_path = f"{self.model_dir}/{anomaly_type}_svm.joblib"
            if os.path.exists(model_path):
                self.models[anomaly_type] = joblib.load(model_path)
            
            # Feature scaler ë¡œë“œ
            scaler_path = f"{self.model_dir}/{anomaly_type}_scaler.joblib"
            if os.path.exists(scaler_path):
                self.scalers[anomaly_type] = joblib.load(scaler_path)
            
            # Feature extractor ì´ˆê¸°í™” (ë™ì¼í•œ ì„¤ì •ìœ¼ë¡œ)
            self.extractors[anomaly_type] = self._create_extractor(anomaly_type)
    
    def predict_anomalies(self, epc_code: str, epc_group: pd.DataFrame) -> Dict[str, float]:
        """ìƒˆë¡œìš´ EPCì— ëŒ€í•œ ì´ìƒì¹˜ ì˜ˆì¸¡"""
        predictions = {}
        
        for anomaly_type, model in self.models.items():
            # 1. Feature ì¶”ì¶œ (í›ˆë ¨ì‹œì™€ ë™ì¼í•œ ë°©ì‹)
            if anomaly_type == 'epcFake':
                features = self.extractors[anomaly_type].extract_features(epc_code)
            else:
                features = self.extractors[anomaly_type].extract_features(epc_group)
            
            # 2. Feature ì •ê·œí™” (í›ˆë ¨ì‹œì™€ ë™ì¼í•œ scaler ì‚¬ìš©)
            features_array = np.array(features).reshape(1, -1)
            if anomaly_type in self.scalers:
                features_scaled = self.scalers[anomaly_type].transform(features_array)
            else:
                features_scaled = features_array
            
            # 3. SVM ì˜ˆì¸¡
            probability = model.predict_proba(features_scaled)[0][1]  # ì´ìƒì¹˜ í™•ë¥ 
            predictions[anomaly_type] = float(probability)
        
        return predictions
    
    def batch_predict(self, df: pd.DataFrame) -> List[Dict]:
        """ì—¬ëŸ¬ EPCì— ëŒ€í•œ ë°°ì¹˜ ì˜ˆì¸¡"""
        results = []
        
        # ê¸°ì¡´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì¬ì‚¬ìš©
        df_clean = preprocess_scan_data(df)
        
        for epc_code, epc_group in df_clean.groupby('epc_code'):
            epc_group = epc_group.sort_values('event_time').reset_index(drop=True)
            predictions = self.predict_anomalies(epc_code, epc_group)
            
            results.append({
                'epc_code': epc_code,
                'predictions': predictions,
                'max_anomaly_score': max(predictions.values()),
                'primary_anomaly': max(predictions.items(), key=lambda x: x[1])[0]
            })
        
        return results

# í†µí•© ì›Œí¬í”Œë¡œìš° ì˜ˆì‹œ
class SVMWorkflow:
    """ì „ì²˜ë¦¬ â†’ í›ˆë ¨ â†’ ì˜ˆì¸¡ì˜ ì „ì²´ ì›Œí¬í”Œë¡œìš°"""
    
    def train_models(self, training_data_path: str):
        """1ë‹¨ê³„: ëª¨ë¸ í›ˆë ¨"""
        # ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        pipeline = SVMPreprocessingPipeline()
        raw_df = pd.read_csv(training_data_path)
        training_data = pipeline.process_data(raw_df)
        
        # SVM ëª¨ë¸ í›ˆë ¨
        from sklearn.svm import SVC
        for anomaly_type, (X, y) in training_data.items():
            # í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬
            X_balanced, y_balanced = self._handle_imbalance(X, y)
            
            # SVM í›ˆë ¨
            model = SVC(probability=True, class_weight='balanced')
            model.fit(X_balanced, y_balanced)
            
            # ëª¨ë¸ ì €ì¥
            joblib.dump(model, f"models/{anomaly_type}_svm.joblib")
    
    def predict_new_data(self, new_data_path: str):
        """2ë‹¨ê³„: ìƒˆë¡œìš´ ë°ì´í„° ì˜ˆì¸¡"""
        engine = SVMInferenceEngine("models/")
        new_df = pd.read_csv(new_data_path)
        return engine.batch_predict(new_df)
ğŸ“‹ ìˆ˜ì •ëœ êµ¬í˜„ ìš°ì„ ìˆœìœ„
ë‹¨ê³„	ì‘ì—…	ì˜ˆìƒ ì‹œê°„	ì¤‘ìš”ë„
1	Feature ì°¨ì› í‘œì¤€í™”	2ì‹œê°„	ğŸ”´ í•„ìˆ˜
2	í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬	3ì‹œê°„	ğŸ”´ í•„ìˆ˜
3	Feature ì •ê·œí™” ì¶”ê°€	1.5ì‹œê°„	ğŸ”´ í•„ìˆ˜
4	ë°°ì¹˜ ì²˜ë¦¬ êµ¬í˜„	2.5ì‹œê°„	ğŸŸ¡ ì„±ëŠ¥
5	ì˜ˆì¸¡ ì—”ì§„ êµ¬í˜„	3ì‹œê°„	ğŸ”´ í•„ìˆ˜
ğŸ¯ í•µì‹¬ ë³€ê²½ì‚¬í•­ ìš”ì•½
ì°¨ì› í†µì¼: ëª¨ë“  feature extractorê°€ ê³ ì • ê¸¸ì´ ë²¡í„° ë°˜í™˜
ë¶ˆê· í˜• í•´ê²°: SMOTE, class_weight, threshold tuning ì˜µì…˜ ì œê³µ
ì •ê·œí™” í•„ìˆ˜: SVM íŠ¹ì„±ìƒ feature scaling ë°˜ë“œì‹œ ì ìš©
ë©”ëª¨ë¦¬ ìµœì í™”: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°°ì¹˜ ì²˜ë¦¬
ì˜ˆì¸¡ ì—°ë™: í›ˆë ¨ëœ ëª¨ë¸ë¡œ ì‹¤ì œ ì˜ˆì¸¡í•˜ëŠ” ì „ì²´ ì›Œí¬í”Œë¡œìš°
ì´ëŸ¬í•œ ìˆ˜ì •ì‚¬í•­ë“¤ì´ ë°˜ì˜ë˜ë©´ ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ë™ì‘í•˜ëŠ” SVM ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

í•­ë“±ì› 1 ì´ë‚˜ 0 ì „ì²´ í”¼ì³ì—ì„œ ì–˜ê°€ 0ì´ë¼ë„ ì „ì²´ ì„ í˜•ì‹œìŠ¤í…œì— ì˜í–¥ì„ ì•ˆì£¼ë‹¤ê³  ìƒê°í•´ì„œ 0ìœ¼ë¡œ ê³±í•˜ëŠ”ê°€ ? ì‹œí€€ìŠ¤ì— ê¸¸ì´ ì•ˆë§ëŠ”ë° 0ì„  (ë¶€í˜¸ê°€ ë°”ë€Œì–´ì„œ ë¬´ì‹œì‹œí‚¤ëŠ” -1)

ë¹ˆê°’ì´ í•œê³³ì— ëª¨ì—¬ì‡ëŠ”ê±° ì•ˆìë¦„, ë¹ˆê²Œ ë¬´ì‘ìœ„ë¡œ ë¶„ì‚°ë˜ì–´ì‡ì–´ì•¼í•¨, ê·œì¹™ì„±ì‡ëŠ”ê±° ì•ˆìë¦„, ê·œì¹™ì—ë§ˆã…£ã…ˆì¶°ê°’ì„ë„£ìŒ,ìë¥´ëŠ”ê²Œ 3í¼ ë„˜ìœ¼ë©´ ì•ˆìë¥´ê³  ë³´ê°„í•¨. 
ví‰ê· ê³¼ ì¤‘ì•™ê°’ì„ íŒë‹¨í•˜ê³  í”¼ì³ë¥¼ ìë¥¼ê±¸ ê³ ë ¤í•˜ê¸° 
íŒë‹¨ê¸°ì¤€ì´ ì—†ìœ¼ë©´ ìë¥¸ë‹¤ëŠ” ê²°ì •ì„ ëª»í•¨

ì´ê±° ë°˜ì˜í–ˆë‹ˆ?


ë¬¸ì œ 2: ì •ê·œí™” ëˆ„ë½ìœ¼ë¡œ ì¸í•œ Feature ë¬´ì‹œ
í° ê°’ì´ ì‘ì€ ê°’ì„ ì§€ë°°í•˜ëŠ” ë¬¸ì œ
ì‹¤ì œ Feature ìŠ¤ì¼€ì¼ ì°¨ì´
python# EPC Fake íŠ¹ì„± ì˜ˆì‹œ
epc_features = [
    1.0,          # boolean: valid_structure (0-1 ë²”ìœ„)
    0.0,          # boolean: valid_header (0-1 ë²”ìœ„)  
    1.0,          # boolean: valid_company (0-1 ë²”ìœ„)
    47.0,         # length: epc_code_length (10-100 ë²”ìœ„)
    0.000234,     # entropy: information_entropy (0-1 ë²”ìœ„)
    1672531200,   # timestamp: manufacture_date (Unix timestamp)
    156.8,        # count: character_count (0-200 ë²”ìœ„)
    0.85          # ratio: digit_ratio (0-1 ë²”ìœ„)
]

# SVMì´ ë³´ëŠ” ê²ƒ:
# "1672531200ì´ ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì„±ì´êµ¬ë‚˜! ë‚˜ë¨¸ì§€ëŠ” ë¬´ì‹œ!"
SVM ê±°ë¦¬ ê³„ì‚°ì˜ ì™œê³¡
python# SVMì€ ë°ì´í„° í¬ì¸íŠ¸ ê°„ ê±°ë¦¬ë¥¼ ê³„ì‚°í•¨
# ìœ í´ë¦¬ë“œ ê±°ë¦¬: sqrt((x1-x2)Â² + (y1-y2)Â² + ...)

ì •ìƒ EPC: [1.0, 0.0, 1.0, 47.0, 0.000234, 1672531200, 156.8, 0.85]
ì´ìƒ EPC: [0.0, 0.0, 0.0, 45.0, 0.000198, 1672531150, 154.2, 0.82]

ê±°ë¦¬ ê³„ì‚°:
= sqrt((1-0)Â² + (0-0)Â² + (1-0)Â² + (47-45)Â² + (0.000234-0.000198)Â² + (1672531200-1672531150)Â² + (156.8-154.2)Â² + (0.85-0.82)Â²)
= sqrt(1 + 0 + 1 + 4 + 0.000000001 + 2500 + 6.76 + 0.0009)
= sqrt(2512.76) â‰ˆ 50.13

# í•˜ì§€ë§Œ ì§„ì§œ ê¸°ì—¬ë„:
timestamp ì°¨ì´: sqrt(2500) = 50.0   â† 99% ê¸°ì—¬
ë‚˜ë¨¸ì§€ ëª¨ë“  íŠ¹ì„±: sqrt(12.76) = 3.6  â† 1% ê¸°ì—¬
ì¤‘ìš”í•œ íŒ¨í„´ì´ ë¬»í˜€ë²„ë¦¼
python# ì‹¤ì œë¡œ ì¤‘ìš”í•œ íŒ¨í„´ë“¤
ì •ìƒ EPC: booleanë“¤ì´ ëª¨ë‘ 1 (ì™„ë²½í•œ í˜•ì‹)
ì´ìƒ EPC: booleanë“¤ì´ ëª¨ë‘ 0 (í˜•ì‹ ì˜¤ë¥˜)

# í•˜ì§€ë§Œ SVMì€ ì´ê²ƒë§Œ ë´„:
ì •ìƒ EPC: timestamp = 1672531200
ì´ìƒ EPC: timestamp = 1672531150
# "ì•„, 50ì´ˆ ì°¨ì´êµ¬ë‚˜. ì´ê±¸ë¡œ ë¶„ë¥˜í•˜ì!"

# ê²°ê³¼: ì§„ì§œ ì¤‘ìš”í•œ EPC í˜•ì‹ ì˜¤ë¥˜ëŠ” ë¬´ì‹œë˜ê³ 
#       ì˜ë¯¸ì—†ëŠ” ì œì¡°ì‹œê°„ ì°¨ì´ë¡œë§Œ ë¶„ë¥˜í•¨

ğŸ’¡ í•´ê²°ì±…ê³¼ íš¨ê³¼
1. í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°
SMOTE (Synthetic Minority Oversampling)
pythonfrom imblearn.over_sampling import SMOTE

# ì›ë³¸ ë°ì´í„°
ì •ìƒ: 99,000ê°œ
ì´ìƒ: 1,000ê°œ

# SMOTE ì ìš© í›„
smote = SMOTE(random_state=42)
X_balanced, y_balanced = smote.fit_resample(X_train, y_train)

ì •ìƒ: 99,000ê°œ
ì´ìƒ: 99,000ê°œ (ì¸ê³µ ìƒì„±)

# ê²°ê³¼: SVMì´ ì´ìƒì¹˜ë¥¼ ì§„ì§œë¡œ í•™ìŠµí•¨!
Class Weight ì¡°ì •
python# ë¶ˆê· í˜•ì„ í˜ë„í‹°ë¡œ ë³´ì •
svm = SVC(class_weight='balanced')  
# ë‚´ë¶€ì ìœ¼ë¡œ: 
# ì •ìƒ ì˜¤ë¶„ë¥˜ í˜ë„í‹°: 1.0
# ì´ìƒ ì˜¤ë¶„ë¥˜ í˜ë„í‹°: 99.0 (99ë°° ë” ì¤‘ìš”í•˜ê²Œ!)
2. ì •ê·œí™”ë¡œ Feature ìŠ¤ì¼€ì¼ í†µì¼
StandardScaler ì ìš©
pythonfrom sklearn.preprocessing import StandardScaler

# ì •ê·œí™” ì „
features_raw = [1.0, 0.0, 1.0, 47.0, 0.000234, 1672531200, 156.8, 0.85]

# ì •ê·œí™” í›„ (í‰ê· =0, í‘œì¤€í¸ì°¨=1)
scaler = StandardScaler()
features_scaled = scaler.fit_transform([features_raw])
# â†’ [0.23, -1.41, 0.23, 0.15, -0.89, 1.34, 0.67, 0.12]

# ì´ì œ ëª¨ë“  íŠ¹ì„±ì´ ë™ë“±í•œ ë²”ìœ„ (-3 ~ +3)ì—ì„œ ê²½ìŸ!
RobustScaler (ì¶”ì²œ)
pythonfrom sklearn.preprocessing import RobustScaler

# ì´ìƒì¹˜ì— ëœ ë¯¼ê°í•œ ì •ê·œí™”
scaler = RobustScaler()  # ì¤‘ì•™ê°’ê³¼ IQR ì‚¬ìš©
features_robust = scaler.fit_transform([features_raw])

# ê·¹ë‹¨ì ì¸ timestamp ê°’ì´ ìˆì–´ë„ ì•ˆì •ì ìœ¼ë¡œ ì •ê·œí™”

. í•´ê²°ì±…: ì •ê·œí™” (Feature Scaling)
ì •ê·œí™”ëŠ” ë§ ê·¸ëŒ€ë¡œ ê°’ì˜ í¬ê¸°ë¥¼ ì¤„ì—¬ì„œ ëª¨ë‘ ë¹„ìŠ·í•˜ê²Œ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤.

ğŸ“ ì˜ˆì‹œ: StandardScaler
ëª¨ë“  ê°’ì„ í‰ê·  0, í‘œì¤€í¸ì°¨ 1ë¡œ ë³€í™˜

python
ë³µì‚¬
í¸ì§‘
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
features_scaled = scaler.fit_transform([features])
ğŸ“ ì˜ˆì‹œ: RobustScaler (ìš°ë¦¬ê°€ ì¶”ì²œ)
í‰ê·  ëŒ€ì‹  ì¤‘ì•™ê°’, í‘œì¤€í¸ì°¨ ëŒ€ì‹  IQR ì‚¬ìš© â†’ ì´ìƒì¹˜ì— ê°•í•¨

python
ë³µì‚¬
í¸ì§‘
from sklearn.preprocessing import RobustScaler
scaler = RobustScaler()
features_robust = scaler.fit_transform([features])
â†’ ì´ëŸ¬ë©´ ë‹¤ìŒì²˜ëŸ¼ ì „ë¶€ ê°’ì´ -3 ~ +3 ì‚¬ì´ë¡œ ë°”ë€œ
â†’ ëª¨ë“  í”¼ì²˜ê°€ "ê³µì •í•˜ê²Œ" ë¹„êµë¨



ì´ê±° ê³ ë ¤í–‡ë‹ˆ