### **LSTM Implementation Plan - DEFERRED**

**⚠️ PRIORITY UPDATE:** Based on assessment findings, LSTM implementation is **deferred** until existing systems are stabilized.

**Current Status:** This plan remains technically sound but is strategically premature given:
- SVM system has unresolved bugs (feature extraction, JSON serialization)
- Rule-based detection achieving only 56.6% accuracy (missing 43% of anomalies)
- Both systems need fixes before adding new complexity

**Recommended Priority Order:**
1. **Fix SVM bugs first** (highest priority) - resolve feature extraction and JSON serialization
2. **Improve rule-based detection** (medium priority) - fix jump/event order detection (currently 0%)
3. **Then consider LSTM** (future) - only after establishing solid baselines

---

### **Phase 1: Data Pipeline & Feature Engineering (Adhering to `data_requirements`)**

1.  **Data Aggregation & Enrichment:**
    *   Create a new script, `src/barcode/lstm_data_preprocessor.py`.
    *   This script will merge the raw training data (`data/raw/*.csv`).
    *   It will then enrich the merged data by joining it with the required processed files:
        *   `data/processed/location_id_withGeospatial.csv`
        *   `data/processed/business_step_transition_avg_v2.csv`
    *   The data will be sorted chronologically by `event_time` to prepare for sequencing.

2.  **Rule-Based Labeling:**
    *   Integrate the `MultiAnomalyDetector` from `src/barcode/multi_anomaly_detector.py` to generate the 5 binary ground-truth labels (`epcFake`, `epcDup`, `locErr`, `evtOrderErr`, `jump`) for each row.

3.  **Sequence Generation:**
    *   Group the labeled data by `epc_code`.
    *   Define a fixed `sequence_length` (e.g., 10, to be finalized in Q&A).
    *   Generate sequences for each EPC. Use zero-padding for sequences shorter than `sequence_length`.
    *   The target label for each sequence will be the 5-dimensional label vector of the *last* event in that sequence.

4.  **Data Splitting & Saving (tt.txt compliant):**
    *   Implement a chronological train/evaluation split (80/20) based on the timestamp of the last event in each sequence.
    *   Save the final, processed sequences and labels into a dedicated folder: `data/lstm_training/`.

---

### **Phase 2: LSTM Model Architecture (Following `ai_guidelines`)**

1.  **Model Definition (`src/barcode/lstm_detector.py`):**
    *   This new file will define the LSTM model using `torch`, leveraging the existing CUDA-enabled environment.
    *   **Architecture:**
        *   Input Layer (accepting sequences of shape `(batch_size, sequence_length, num_features)`)
        *   Two stacked `nn.LSTM` layers for robust pattern detection.
        *   `nn.Dropout` layers between LSTM layers and before the final output layer to prevent overfitting.
        *   A final `nn.Linear` layer with 5 output neurons.
        *   A `nn.Sigmoid` activation function to produce the 5 multi-label probability scores.
    *   The code will be clear and well-commented, explaining the "why" behind architectural choices.

2.  **Training Script (`train_lstm_model.py`):**
    *   This new script will orchestrate the training process.
    *   It will load the prepared data from `data/lstm_training/`.
    *   The model will be trained using `binary_crossentropy` loss and the `Adam` optimizer.
    *   The script will include GPU utilization logic (`.to(device)`).
    *   The final trained model will be saved to `models/lstm/lstm_model_YYYYMMDD.pt`.

---

### **Phase 3: API Integration (Modifying `fastapi_server.py`)**

1.  **Create a New Endpoint:**
    *   To maintain clarity and versioning, a new endpoint will be added: `POST /api/manager/export-and-analyze-async/lstm`.
    *   This keeps the production-ready rule-based endpoint and the experimental SVM endpoint separate.

2.  **Inference Logic:**
    *   The endpoint will load the trained LSTM model (`.pt` file) once at startup.
    *   For each incoming request, it will perform the necessary preprocessing and sequence construction in real-time. This is a critical step and will require careful implementation to meet the <7s performance target.
    *   The prepared sequence will be fed to the LSTM model for inference.
    *   The output will be formatted into the standard JSON response, including the `lstmScore` for each detected anomaly.

---

### **Phase 4: Evaluation & Monitoring**

1.  **Evaluation Script (`evaluate_lstm_model.py`):**
    *   This new script will be created, following the academic rigor of `evaluate_svm_models.py`.
    *   It will load the evaluation dataset from `data/lstm_training/`.
    *   It will calculate and compare the performance (Recall, Precision, F1, Specificity) of the LSTM model against the rule-based system.
    *   Results will be logged to `logs/lstm_models/lstm_performance_log.csv`.

---

### **Critical Issues to Address Before LSTM Implementation**

**Technical Challenges Identified:**

1. **Real-time Sequence Construction:** Building input sequences requires fetching last N-1 events for each EPC within <7s API requirement. No clear solution for this fundamental lookup architecture challenge.

2. **Cold Start Problem:** First events for any EPC have no historical sequence (padding-only input), requiring fallback to rule-based detection and adding system complexity.

3. **Performance vs Benefit Unclear:** Rule-based system works and is fast, SVM framework exists but needs fixes. ROI for LSTM complexity investment is uncertain.

### **Future LSTM Questions (When Ready)**

1. **Sequence Length:** Optimal `sequence_length` balancing temporal patterns vs performance (recommendation: 10-15)
2. **Caching Strategy:** Redis for recent events vs querying primary data store for real-time sequence construction  
3. **Cold Start Handling:** Low confidence scores vs rule-based fallback for first few events
4. **Architecture Complexity:** Two-layer vs single-layer LSTM performance trade-offs
5. **API Batching:** Processing events in batches vs individually for efficiency

**Next Action Required:** Complete SVM bug fixes and rule-based improvements before revisiting this plan.