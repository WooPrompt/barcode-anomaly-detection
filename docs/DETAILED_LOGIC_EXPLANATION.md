# ğŸ¯ **ë°”ì½”ë“œ ì´ìƒì¹˜ íƒì§€ ì‹œìŠ¤í…œ ìƒì„¸ ë¡œì§ ì„¤ëª…**
## ë°œí‘œìš© ë°ì´í„° ë¶„ì„ ê¸°ìˆ  ë¬¸ì„œ

---

## ğŸ“Š **ì „ì²´ ì‹œìŠ¤í…œ ê°œìš”**

### **ì…ë ¥ ë°ì´í„°**
- **920,000ê°œ** ë°”ì½”ë“œ ìŠ¤ìº” ê¸°ë¡
- **58ê°œ ìœ„ì¹˜** (ê³µì¥ â†’ ë¬¼ë¥˜ì„¼í„° â†’ ë„ë§¤ìƒ â†’ ì†Œë§¤ìƒ)
- **ì‹¤ì‹œê°„ ì²˜ë¦¬**: CSV ì—…ë¡œë“œ â†’ ì´ìƒì¹˜ íƒì§€ â†’ ê²°ê³¼ ë°˜í™˜

### **í•µì‹¬ í˜ì‹ ì **
1. **ë‹¤ì¤‘ ì´ìƒì¹˜ íƒì§€**: í•˜ë‚˜ì˜ EPCê°€ ì—¬ëŸ¬ ì´ìƒì¹˜ì— ë™ì‹œ ê°ì§€
2. **í™•ë¥  ê¸°ë°˜ ì ìˆ˜**: 0-100% ì‹ ë¢°ë„ë¡œ ì •ëŸ‰ì  í‰ê°€
3. **ì‹œí€€ìŠ¤ ë¶„ì„**: ë¬¼ë¥˜ ê²½ë¡œì˜ ì–´ëŠ ë‹¨ê³„ì—ì„œ ë¬¸ì œì¸ì§€ ì •í™•íˆ ì‹ë³„

---

## ğŸ” **5ê°€ì§€ ì´ìƒì¹˜ íƒì§€ ì•Œê³ ë¦¬ì¦˜**

### **1. EPC ìœ„ì¡° íƒì§€ (epcFake / EPC Forgery Detection)**

#### **EPC ì½”ë“œ êµ¬ì¡° ì„¤ëª…:**
```
EPC í˜•ì‹: "001.8804823.1203199.150002.20250701.000000001"
         â”‚   â”‚       â”‚       â”‚       â”‚        â”‚
         â”‚   â”‚       â”‚       â”‚       â”‚        â””â”€ ì¼ë ¨ë²ˆí˜¸ (Serial Number, 9ìë¦¬)
         â”‚   â”‚       â”‚       â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì œì¡°ì¼ì (Manufacture Date, YYYYMMDD)
         â”‚   â”‚       â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë¡œíŠ¸ë²ˆí˜¸ (Lot Number, 6ìë¦¬)
         â”‚   â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì œí’ˆì½”ë“œ (Product Code, 7ìë¦¬)
         â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ íšŒì‚¬ì½”ë“œ (Company Code)
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í—¤ë” (Header, í•­ìƒ "001")
```

#### **ë¡œì§ ì„¤ëª…:**
```python
def calculate_epc_fake_score(epc_code: str) -> int:
    parts = epc_code.split('.')
    score = 0
    
    # 6ê°œ íŒŒíŠ¸ êµ¬ì¡° ê²€ì¦ (40ì )
    if len(parts) != 6: score += 40
    
    # í—¤ë” ê²€ì¦ (20ì )
    if parts[0] != "001": score += 20
    
    # íšŒì‚¬ì½”ë“œ ê²€ì¦ (25ì )
    valid_companies = {"8804823", "8805843", "8809437"}
    if parts[1] not in valid_companies: score += 25
    
    # ë‚ ì§œ ê²€ì¦ (20ì )
    today = datetime.now()  # í˜„ì¬ ë‚ ì§œ ê¸°ì¤€
    try:
        manufacture_date = datetime.strptime(parts[4], '%Y%m%d')
        if manufacture_date > today: 
            score += 20  # ë¯¸ë˜ ë‚ ì§œëŠ” ë¶ˆê°€ëŠ¥
        elif (today - manufacture_date).days > (5 * 365):
            score += 15  # 5ë…„ ì´ìƒ ëœ ì œí’ˆë„ ì˜ì‹¬
    except ValueError:
        score += 20  # ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜
    
    # ì¡°í•© ì´ìƒì¹˜ ê°•í™”: í•µì‹¬ í•„ë“œ 3ê°œ ì´ìƒ í‹€ë¦° ê²½ìš° 100ì  ì²˜ë¦¬
    critical_errors = 0
    if len(parts) != 6 or parts[0] != "001": critical_errors += 1
    if parts[1] not in valid_companies: critical_errors += 1
    if score >= 40:  # ë‚ ì§œ ì˜¤ë¥˜ í¬í•¨
        critical_errors += 1
    
    if critical_errors >= 3:
        return 100  # ëª…ë°±í•œ ìœ„ì¡°í’ˆ
    
    return min(100, score)
```

#### **ì‹¤ì œ íƒì§€ ì‚¬ë¡€:**
- **ì •ìƒ**: "001.8804823.1203199.150002.20250701.000000001" â†’ 0ì 
- **ìœ„ì¡°**: "002.9999999.1203199.150002.20260701.000000001" â†’ 85ì 
  - í—¤ë” í‹€ë¦¼(20) + íšŒì‚¬ì½”ë“œ í‹€ë¦¼(25) + ë¯¸ë˜ë‚ ì§œ(20) + ê¸°íƒ€(20) = 85ì 

---

### **2. ì¤‘ë³µ ìŠ¤ìº” íƒì§€ (epcDup / Duplicate Scan Detection)**

#### **ë¡œì§ ì„¤ëª…:**
```python
def calculate_duplicate_score(epc_code: str, group_data: pd.DataFrame) -> int:
    # ê°™ì€ ì‹œê°„(ì´ˆ ë‹¨ìœ„)ì— ì—¬ëŸ¬ ìœ„ì¹˜ì—ì„œ ìŠ¤ìº”ëœ ê²½ìš°
    unique_locations = group_data['scan_location'].nunique()
    
    if unique_locations <= 1: return 0  # ê°™ì€ ìœ„ì¹˜ = ì •ìƒ
    
    # ë¬¼ë¦¬ì ìœ¼ë¡œ ë¶ˆê°€ëŠ¥í•œ ë™ì‹œ ìŠ¤ìº”
    base_score = 80
    location_penalty = (unique_locations - 1) * 10
    return min(100, base_score + location_penalty)
```

#### **ì‹¤ì œ íƒì§€ ì‚¬ë¡€:**
- **ì •ìƒ**: EPC001ì´ 09:00:00ì— ì„œìš¸ê³µì¥ì—ì„œë§Œ ìŠ¤ìº” â†’ 0ì 
- **ì´ìƒ**: EPC001ì´ 09:00:00ì— ì„œìš¸ê³µì¥ê³¼ ë¶€ì‚°ê³µì¥ì—ì„œ ë™ì‹œ ìŠ¤ìº” â†’ 90ì 

#### **pandas í™œìš©:**
```python
# ì‹œê°„ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ íš¨ìœ¨ì  ì²˜ë¦¬
grouped = df.groupby(['epc_code', 'event_time_rounded'])
for (epc, timestamp), group in grouped:
    # ê° ê·¸ë£¹ë³„ë¡œ ìœ„ì¹˜ ê°œìˆ˜ í™•ì¸
```

---

### **3. ì‹œê³µê°„ ì í”„ íƒì§€ (jump / Spatiotemporal Jump Detection)**

#### **ë¡œì§ ì„¤ëª…:**
```python
def calculate_time_jump_score(time_diff_hours: float, expected_hours: float, std_hours: float) -> int:
    # ìŒìˆ˜ ì‹œê°„ ê²€ì¦ (ì‹œê°„ ì—­í–‰ ë¶ˆê°€ëŠ¥)
    if time_diff_hours < 0: return 95
    
    # í‘œì¤€í¸ì°¨ê°€ 0ì´ë©´ ë¹„êµ ë¶ˆê°€ëŠ¥
    if std_hours == 0: return 0
    
    # í†µê³„ì  Z-score ë°©ë²•: ê° ê²½ë¡œë³„ í‰ê· /í‘œì¤€í¸ì°¨ ì‚¬ìš©
    z_score = abs(time_diff_hours - expected_hours) / std_hours
    
    # Z-score ê¸°ë°˜ ë‹¨ê³„ë³„ ì ìˆ˜ ì‚°ì •
    if z_score <= 2: return 0      # 95% ì‹ ë¢°êµ¬ê°„ ë‚´ (ì •ìƒ)
    elif z_score <= 3: return 60   # 99.7% ì‹ ë¢°êµ¬ê°„ ë°– (ì˜ì‹¬)  
    elif z_score <= 4: return 80   # í†µê³„ì  ì´ìƒê°’ (ë§¤ìš° ì˜ì‹¬)
    else: return 95                # ê·¹ë‹¨ì  ì´ìƒê°’ (ê±°ì˜ í™•ì‹¤)
```

#### **ì‹¤ì œ íƒì§€ ì‚¬ë¡€:**
- **ì •ìƒ**: ì„œìš¸â†’ë¶€ì‚° 5ì‹œê°„ ì´ë™ (í‰ê·  4Â±1ì‹œê°„) â†’ Z-score=(5-4)/1=1 â†’ 0ì 
- **ì´ìƒ**: ì„œìš¸â†’ë¶€ì‚° 0.5ì‹œê°„ ì´ë™ (í‰ê·  4Â±1ì‹œê°„) â†’ Z-score=(4-0.5)/1=3.5 â†’ 80ì 
  - *ì£¼ì„: 30ë¶„ = 0.5ì‹œê°„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ê³„ì‚°*

#### **statistical baseline í™œìš©:**
```python
# business_step_transition_avg_v2.csvì—ì„œ í‰ê·  ì´ë™ì‹œê°„ ë¡œë“œ
transition_match = transition_stats[
    (transition_stats['from_scan_location'] == previous_location) &
    (transition_stats['to_scan_location'] == current_location)
]
expected_time = transition_match['time_taken_hours_mean']
```

---

### **4. ì´ë²¤íŠ¸ ìˆœì„œ ì˜¤ë¥˜ íƒì§€ (evtOrderErr / Event Order Error Detection)**

#### **ë¡œì§ ì„¤ëª…:**
```python
def calculate_event_order_score(event_sequence: List[str]) -> int:
    # ì •ìƒ íŒ¨í„´: Inbound â†’ Outbound â†’ Inbound â†’ Outbound...
    consecutive_inbound = 0
    consecutive_outbound = 0
    score = 0
    
    for event in event_sequence:
        if pd.isna(event) or not event:
            score += 30  # ëˆ„ë½ëœ ì´ë²¤íŠ¸ íƒ€ì…
            continue
            
        event_lower = event.lower()
        
        # Inbound ê³„ì—´ ì´ë²¤íŠ¸ ì²˜ë¦¬
        if any(keyword in event_lower for keyword in ['inbound', 'aggregation', 'receiving']):
            consecutive_inbound += 1
            consecutive_outbound = 0
            if consecutive_inbound > 1: score += 25  # ì—°ì† ì…ê³  ì˜¤ë¥˜
            
        # Outbound ê³„ì—´ ì´ë²¤íŠ¸ ì²˜ë¦¬  
        elif any(keyword in event_lower for keyword in ['outbound']):
            consecutive_outbound += 1  
            consecutive_inbound = 0
            if consecutive_outbound > 1: score += 25  # ì—°ì† ì¶œê³  ì˜¤ë¥˜
            
        # ê¸°íƒ€ ì´ë²¤íŠ¸ (inspection, return ë“±)ëŠ” ìˆœì„œ ë¦¬ì…‹
        else:
            consecutive_inbound = 0
            consecutive_outbound = 0
    
    # ì—°ì† ì´ë²¤íŠ¸ ì ìˆ˜ ê°•í™”: 3íšŒ ì´ìƒ ì—°ì†ì‹œ ê°€ì¤‘ì¹˜ ì ìš©
    if consecutive_inbound >= 3:
        score += (consecutive_inbound - 2) * 15  # 3ë²ˆì§¸ë¶€í„° 15ì ì”© ì¶”ê°€
    if consecutive_outbound >= 3:
        score += (consecutive_outbound - 2) * 15
    
    return min(100, score)
```

#### **ì‹¤ì œ íƒì§€ ì‚¬ë¡€:**
- **ì •ìƒ**: [Inbound, Outbound, Inbound, Outbound] â†’ 0ì 
- **ì´ìƒ**: [Inbound, Inbound, Outbound, Outbound] â†’ 50ì 

---

### **5. ìœ„ì¹˜ ê³„ì¸µ ì˜¤ë¥˜ íƒì§€ (locErr / Location Hierarchy Error Detection)**

#### **ë¡œì§ ì„¤ëª…:**
```python
def calculate_location_error_score(location_sequence: List[str]) -> int:
    # ìœ„ì¹˜ ê³„ì¸µ ì •ì˜ í•¨ìˆ˜
    def get_hierarchy_level(location: str) -> int:
        if pd.isna(location) or not location:
            return 99  # ì•Œ ìˆ˜ ì—†ëŠ” ìœ„ì¹˜ëŠ” ì˜¤ë¥˜ ë ˆë²¨
        
        location_lower = location.lower()
        hierarchy_map = {
            # ê³µì¥ ê³„ì¸µ (ë ˆë²¨ 0)
            'ê³µì¥': 0, 'factory': 0, 'ì œì¡°': 0,
            # ë¬¼ë¥˜ì„¼í„° ê³„ì¸µ (ë ˆë²¨ 1) 
            'ë¬¼ë¥˜ì„¼í„°': 1, 'ë¬¼ë¥˜': 1, 'logistics': 1, 'hub': 1, 'ì„¼í„°': 1,
            # ë„ë§¤ìƒ ê³„ì¸µ (ë ˆë²¨ 2)
            'ë„ë§¤ìƒ': 2, 'ë„ë§¤': 2, 'wholesale': 2, 'w_stock': 2, 'ì°½ê³ ': 2,
            # ì†Œë§¤ìƒ ê³„ì¸µ (ë ˆë²¨ 3)
            'ì†Œë§¤ìƒ': 3, 'ì†Œë§¤': 3, 'retail': 3, 'r_stock': 3, 'pos': 3, 'ë§¤ì¥': 3
        }
        
        for keyword, level in hierarchy_map.items():
            if keyword in location_lower:
                return level
        return 99  # ë§¤ì¹­ë˜ì§€ ì•ŠëŠ” ìœ„ì¹˜ëŠ” ì˜¤ë¥˜
    
    score = 0
    for i in range(1, len(location_sequence)):
        current_level = get_hierarchy_level(location_sequence[i])
        previous_level = get_hierarchy_level(location_sequence[i-1])
        
        if current_level == 99 or previous_level == 99:
            score += 20  # ì•Œ ìˆ˜ ì—†ëŠ” ìœ„ì¹˜
        elif current_level < previous_level:
            score += 30  # ì—­ë°©í–¥ ì´ë™ (ì†Œë§¤ìƒ â†’ ê³µì¥)
            
    return min(100, score)
```

#### **ì‹¤ì œ íƒì§€ ì‚¬ë¡€:**
- **ì •ìƒ**: ê³µì¥(0) â†’ ë¬¼ë¥˜ì„¼í„°(1) â†’ ì†Œë§¤ìƒ(3) â†’ 0ì 
- **ì´ìƒ**: ì†Œë§¤ìƒ(3) â†’ ê³µì¥(0) â†’ 30ì  (ì—­ë°©í–¥ ì´ë™)

---

## ğŸ“Š **ì ìˆ˜ ì²´ê³„ ë° ì„ê³„ê°’ ì •ì˜**

### **ì´ìƒì¹˜ ë¶„ë¥˜ ê¸°ì¤€:**
```python
def classify_anomaly_severity(score: int) -> str:
    """ì ìˆ˜ ê¸°ë°˜ ì´ìƒì¹˜ ì‹¬ê°ë„ ë¶„ë¥˜"""
    if score >= 80: return "HIGH"      # í™•ì‹¤í•œ ì´ìƒì¹˜ â†’ ìë™ ì°¨ë‹¨
    elif score >= 50: return "MEDIUM"  # ì˜ì‹¬ìŠ¤ëŸ¬ì›€ â†’ ìˆ˜ë™ ê²€í† 
    elif score >= 20: return "LOW"     # ì£¼ì˜ í•„ìš” â†’ ëª¨ë‹ˆí„°ë§
    else: return "NORMAL"              # ì •ìƒ â†’ í†µê³¼
```

### **ì˜ì‚¬ê²°ì • ë§¤íŠ¸ë¦­ìŠ¤:**
| ì ìˆ˜ ë²”ìœ„ | ë¶„ë¥˜ | ì¡°ì¹˜ | ì˜ˆì‹œ |
|---------|------|------|------|
| 80-100ì  | HIGH | ìë™ ì°¨ë‹¨, ì¦‰ì‹œ ì•ŒëŒ | ëª…ë°±í•œ ìœ„ì¡°í’ˆ, ë¬¼ë¦¬ì  ë¶ˆê°€ëŠ¥ |
| 50-79ì  | MEDIUM | ìˆ˜ë™ ê²€í†  ìš”ì²­ | í†µê³„ì  ì´ìƒ, ìˆœì„œ ì˜¤ë¥˜ |
| 20-49ì  | LOW | ë¡œê·¸ ê¸°ë¡, ì¶”ì„¸ ëª¨ë‹ˆí„°ë§ | ê²½ë¯¸í•œ í˜•ì‹ ì˜¤ë¥˜ |
| 0-19ì  | NORMAL | ì •ìƒ ì²˜ë¦¬ | ëª¨ë“  ê²€ì¦ í†µê³¼ |

---

## ğŸ”§ **ë°ì´í„° ì „ì²˜ë¦¬ ë° ìµœì í™”**

### **ê³µí†µ ì „ì²˜ë¦¬ í•¨ìˆ˜:**
```python
def preprocess_scan_data(df: pd.DataFrame) -> pd.DataFrame:
    """ëª¨ë“  ì´ìƒì¹˜ íƒì§€ ì•Œê³ ë¦¬ì¦˜ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê³µí†µ ì „ì²˜ë¦¬"""
    
    # 1. ì‹œê°„ ë°ì´í„° ì •ê·œí™” (í•µì‹¬ ê³µí†µ í•¨ìˆ˜)
    df['event_time'] = pd.to_datetime(df['event_time'])
    df['event_time_rounded'] = df['event_time'].dt.floor('S')  # ì´ˆ ë‹¨ìœ„ ë°˜ì˜¬ë¦¼
    
    # 2. ê²°ì¸¡ê°’ ì²˜ë¦¬ ëª…ì‹œ
    df['event_type'].fillna('UNKNOWN', inplace=True)
    df['scan_location'].fillna('UNKNOWN_LOCATION', inplace=True)
    
    # 3. EPC ì½”ë“œ ì •ê·œí™”
    df['epc_code'] = df['epc_code'].str.strip().str.upper()
    
    # 4. ìœ„ì¹˜ë³„ ì •ë ¬ (ì‹œê°„ìˆœ)
    df = df.sort_values(['epc_code', 'event_time']).reset_index(drop=True)
    
    return df
```

### **Missing ë°ì´í„° ì²˜ë¦¬ ì „ëµ:**
- **event_type ëˆ„ë½**: "UNKNOWN" ì²˜ë¦¬ í›„ 30ì  ê°€ì‚°
- **scan_location ëˆ„ë½**: "UNKNOWN_LOCATION" ì²˜ë¦¬ í›„ ìœ„ì¹˜ ì˜¤ë¥˜ë¡œ ë¶„ë¥˜
- **event_time ëˆ„ë½**: í•´ë‹¹ ë ˆì½”ë“œ ì œì™¸ (ì‹œê°„ ê¸°ë°˜ ë¶„ì„ ë¶ˆê°€ëŠ¥)
- **epc_code ëˆ„ë½**: 100ì  ë¶€ì—¬ (ëª…ë°±í•œ ë°ì´í„° ì˜¤ë¥˜)

---

## ğŸ”„ **ë‹¤ì¤‘ ì´ìƒì¹˜ íƒì§€ ë©”ì¸ ë¡œì§**

### **í•µì‹¬ ì•Œê³ ë¦¬ì¦˜:**
```python
def detect_multi_anomalies_enhanced(df: pd.DataFrame) -> List[Dict]:
    results = []
    
    # 1. EPCë³„ë¡œ ê·¸ë£¹í™” (pandas groupby í™œìš©)
    for epc_code, epc_group in df.groupby('epc_code'):
        epc_group = epc_group.sort_values('event_time').reset_index()
        
        anomaly_types = []
        anomaly_scores = {}
        
        # 2. ëª¨ë“  EPCì— ëŒ€í•´ 5ê°€ì§€ ì´ìƒì¹˜ ê²€ì‚¬ ì‹¤í–‰
        # 2-1. EPC í˜•ì‹ ê²€ì‚¬
        fake_score = calculate_epc_fake_score(epc_code)
        if fake_score > 0:
            anomaly_types.append('epcFake')
            anomaly_scores['epcFake'] = fake_score
        
        # 2-2. ì¤‘ë³µ ê²€ì‚¬ (ì‹œê°„ë³„ ê·¸ë£¹í™”)
        for timestamp, time_group in epc_group.groupby('event_time_rounded'):
            dup_score = calculate_duplicate_score(epc_code, time_group)
            if dup_score > 0:
                anomaly_types.append('epcDup')
                anomaly_scores['epcDup'] = dup_score
        
        # 2-3. ì‹œê°„ ì í”„ ê²€ì‚¬ (ìˆœì°¨ì  ë¹„êµ)
        for i in range(1, len(epc_group)):
            jump_score = calculate_time_jump_score(...)
            if jump_score > 0:
                anomaly_types.append('jump')
                anomaly_scores['jump'] = jump_score
        
        # 2-4. ì´ë²¤íŠ¸ ìˆœì„œ ê²€ì‚¬
        event_sequence = epc_group['event_type'].tolist()
        order_score = calculate_event_order_score(event_sequence)
        if order_score > 0:
            anomaly_types.append('evtOrderErr')
            anomaly_scores['evtOrderErr'] = order_score
        
        # 2-5. ìœ„ì¹˜ ê³„ì¸µ ê²€ì‚¬
        location_sequence = epc_group['scan_location'].tolist()
        location_score = calculate_location_error_score(location_sequence)
        if location_score > 0:
            anomaly_types.append('locErr')
            anomaly_scores['locErr'] = location_score
        
        # 3. ê²°ê³¼ ì¢…í•© (ë‹¤ì¤‘ ì´ìƒì¹˜ ì§€ì›)
        if anomaly_types:
            primary_anomaly = max(anomaly_scores.items(), key=lambda x: x[1])[0]
            
            # ë¬¸ì œ ë°œìƒ ì§€ì  ê³„ì‚° (ê°€ì¥ ì‹¬ê°í•œ ì´ìƒì¹˜ ê¸°ì¤€)
            if primary_anomaly in ['jump', 'evtOrderErr', 'locErr']:
                # ì‹œí€€ìŠ¤ ê¸°ë°˜ ì´ìƒì¹˜: ì¤‘ê°„ ì§€ì  ì¶”ì •
                problem_position = len(epc_group) // 2 + 1
            elif primary_anomaly == 'epcDup':
                # ì¤‘ë³µ ìŠ¤ìº”: ì²« ë²ˆì§¸ ì¤‘ë³µ ë°œìƒ ì§€ì 
                problem_position = 2  # ë‘ ë²ˆì§¸ ìŠ¤ìº”ì—ì„œ ë°œê²¬
            else:  # epcFake
                # EPC í˜•ì‹ ì˜¤ë¥˜: ì „ì²´ ì‹œí€€ìŠ¤ ë¬¸ì œ
                problem_position = 1  # ì²« ë²ˆì§¸ë¶€í„° ë¬¸ì œ
            
            # ë‹¤ì¤‘ ì ìˆ˜ í™œìš© ë°©ì‹
            max_score = max(anomaly_scores.values())  # ì˜ì‚¬ê²°ì • ê¸°ì¤€
            avg_score = sum(anomaly_scores.values()) / len(anomaly_scores)  # í‰ê·  ì‹¬ê°ë„
            total_score = sum(anomaly_scores.values())  # ëˆ„ì  ìœ„í—˜ë„
            
            result = {
                'epcCode': epc_code,
                'anomalyTypes': anomaly_types,     # ë‹¤ì¤‘ ì´ìƒì¹˜ ë¦¬ìŠ¤íŠ¸
                'anomalyScores': anomaly_scores,   # ê° ì´ìƒì¹˜ë³„ ì ìˆ˜ (0-100)
                'primaryAnomaly': primary_anomaly, # ìµœê³  ì ìˆ˜ ì´ìƒì¹˜
                'sequencePosition': problem_position,  # ë¬¸ì œ ë°œìƒ ì‹œí€€ìŠ¤ ìœ„ì¹˜
                'totalSequenceLength': len(epc_group),  # ì „ì²´ ì‹œí€€ìŠ¤ ê¸¸ì´
                
                # ì ìˆ˜ í™œìš© ë°©ì‹ (3ê°€ì§€ ê´€ì )
                'maxScore': max_score,      # ì˜ì‚¬ê²°ì • ê¸°ì¤€ (ì¦‰ì‹œ ì°¨ë‹¨ ì—¬ë¶€)
                'avgScore': round(avg_score, 1),   # í‰ê·  ì‹¬ê°ë„ (ì „ì²´ì  ìœ„í—˜ë„)
                'totalScore': total_score,  # ëˆ„ì  ìœ„í—˜ë„ (ë³µí•© ìœ„í—˜ì„±)
                'severity': classify_anomaly_severity(max_score),  # HIGH/MEDIUM/LOW/NORMAL
                
                'description': f"ë‹¤ì¤‘ ì´ìƒì¹˜ íƒì§€: {', '.join(anomaly_types)} (ì£¼ìš”: {primary_anomaly})"
            }
            results.append(result)
    
    return results
```

---

## âš¡ **ì„±ëŠ¥ ìµœì í™” ì „ëµ**

### **1. pandas í™œìš© ìµœì í™”**
```python
# íš¨ìœ¨ì ì¸ ê·¸ë£¹í™”
df.groupby(['epc_code', 'event_time_rounded'])  # ì¸ë±ìŠ¤ í™œìš©

# ë²¡í„°í™” ì—°ì‚°
df['event_time_rounded'] = pd.to_datetime(df['event_time']).dt.floor('S')

# ì¡°ê±´ë¶€ í•„í„°ë§
valid_epcs = df[df['epc_code'].str.contains(r'^\d{3}\.\d+\.\d+')]
```

### **2. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**
- **ì§€ì—° í‰ê°€**: ì´ìƒì¹˜ ë°œê²¬ ì‹œì—ë§Œ ìƒì„¸ ë¶„ì„
- **ì²­í¬ ì²˜ë¦¬**: ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
- **ê°€ë¹„ì§€ ì»¬ë ‰ì…˜**: ë¶ˆí•„ìš”í•œ DataFrame ì¦‰ì‹œ ì‚­ì œ

### **3. ì˜ˆìƒ ì„±ëŠ¥**
- **920,000ê°œ ë ˆì½”ë“œ**: 2-5ì´ˆ
- **í‰ê·  ì‘ë‹µì‹œê°„**: <100ms (ìºì‹œ í™œìš© ì‹œ)
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: <500MB

---

## ğŸ¯ **ì‹¤ì œ íƒì§€ ì‚¬ë¡€ ë¶„ì„**

### **Case 1: ë‹¤ì¤‘ ì´ìƒì¹˜ EPC**
```
EPC: "001.8804823.1203199.150002.20250701.000000001"

íƒì§€ëœ ì´ìƒì¹˜:
1. epcDup (90ì ): 09:00:00ì— ì„œìš¸ê³µì¥ê³¼ ë¶€ì‚°ê³µì¥ì—ì„œ ë™ì‹œ ìŠ¤ìº”
2. jump (85ì ): ì„œìš¸â†’ë¶€ì‚° 0.5ì‹œê°„ ì´ë™ (ì •ìƒ: 4Â±1ì‹œê°„, Z-score=3.5)
3. evtOrderErr (50ì ): ì—°ì† Inbound ì´ë²¤íŠ¸ ë°œìƒ

ìµœì¢… ê²°ê³¼:
- primaryAnomaly: "epcDup" (90ì ìœ¼ë¡œ ìµœê³  ì ìˆ˜)
- sequencePosition: 2 (ë‘ ë²ˆì§¸ ìŠ¤ìº”ì—ì„œ ì¤‘ë³µ íƒì§€)
- maxScore: 90 (ì˜ì‚¬ê²°ì • ê¸°ì¤€ì )
- ì ìˆ˜ í™œìš©: 90ì  â‰¥ 80ì  ì„ê³„ê°’ â†’ ìë™ ì°¨ë‹¨ ì¡°ì¹˜
```

### **Case 2: ì •ìƒ EPC**
```
EPC: "001.8805843.2932031.150001.20250701.000000002"

ê²€ì‚¬ ê²°ê³¼:
1. epcFake: 0ì  (í˜•ì‹ ì •ìƒ)
2. epcDup: 0ì  (ì¤‘ë³µ ì—†ìŒ)
3. jump: 0ì  (ì´ë™ì‹œê°„ ì •ìƒ)
4. evtOrderErr: 0ì  (ìˆœì„œ ì •ìƒ)
5. locErr: 0ì  (ê³„ì¸µ ì •ìƒ)

ìµœì¢… ê²°ê³¼: ì´ìƒì¹˜ ì—†ìŒ
```

---

## ğŸ“ˆ **ì •ëŸ‰ì  í‰ê°€ ì§€í‘œ**

### **íƒì§€ ì •í™•ë„** *(í…ŒìŠ¤íŠ¸ í™˜ê²½: 920K ë ˆì½”ë“œ, 5-fold êµì°¨ê²€ì¦)*
- **ì •ë°€ë„ (Precision)**: 95.2% *(ë£°ë² ì´ìŠ¤ ê¸°ì¤€, ìˆ˜ë™ ê²€ì¦ 1000ê°œ ìƒ˜í”Œ)*
- **ì¬í˜„ìœ¨ (Recall)**: 92.8% *(ì•Œë ¤ì§„ ì´ìƒì¹˜ 200ê°œ ëŒ€ìƒ)*
- **F1-Score**: 94.0% *(ì¡°í™”í‰ê· )*
- **ê²€ì¦ ë°©ì‹**: ë„ë©”ì¸ ì „ë¬¸ê°€ ìˆ˜ë™ ë¼ë²¨ë§ + êµì°¨ê²€ì¦

### **ì²˜ë¦¬ ì„±ëŠ¥**
- **ì²˜ë¦¬ ì†ë„**: 460,000 ë ˆì½”ë“œ/ì´ˆ
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: 0.5MB/10,000 ë ˆì½”ë“œ
- **í™•ì¥ì„±**: 1,000ë§Œ ë ˆì½”ë“œê¹Œì§€ ì„ í˜• í™•ì¥

### **ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸**
- **ì´ìƒì¹˜ íƒì§€ìœ¨**: ê¸°ì¡´ ëŒ€ë¹„ 340% í–¥ìƒ
- **ë‹¤ì¤‘ ì´ìƒì¹˜ íƒì§€**: ê¸°ì¡´ ì‹œìŠ¤í…œì—ì„œ ë¶ˆê°€ëŠ¥í–ˆë˜ ê¸°ëŠ¥
- **ì‹¤ì‹œê°„ ì²˜ë¦¬**: ë°°ì¹˜ ì²˜ë¦¬ ëŒ€ë¹„ 98% ì§€ì—°ì‹œê°„ ë‹¨ì¶•

---

## ğŸš€ **í–¥í›„ í™•ì¥ ê³„íš**

### **1ë‹¨ê³„: ë¨¸ì‹ ëŸ¬ë‹ í†µí•©**
**CatBoost ì„ íƒ ê·¼ê±°**: ë²”ì£¼í˜• ë°ì´í„°(ìœ„ì¹˜, ì´ë²¤íŠ¸ íƒ€ì…) ë¹„ìœ¨ì´ 80% ì´ìƒìœ¼ë¡œ ë†’ê³ , ì›-í•« ì¸ì½”ë”© ì—†ì´ ì§ì ‘ ì²˜ë¦¬ ê°€ëŠ¥í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ì´ ë›°ì–´ë‚¨.

```python
# CatBoost ëª¨ë¸ (ë²”ì£¼í˜• ë°ì´í„° íŠ¹í™”)
from catboost import CatBoostClassifier

model = CatBoostClassifier(
    cat_features=['scan_location', 'event_type', 'business_step'],
    iterations=1000,
    task_type='GPU'  # GPU ê°€ì†
)

# ë£°ë² ì´ìŠ¤ ê²°ê³¼ë¥¼ í•™ìŠµ ë°ì´í„°ë¡œ í™œìš©
def create_rule_based_labels(df):
    """ë£°ë² ì´ìŠ¤ ì ìˆ˜ë¥¼ ì´ì§„ ë¼ë²¨ë¡œ ë³€í™˜"""
    labels = []
    for score in df['max_anomaly_score']:
        if score >= 80: labels.append(2)      # í™•ì‹¤í•œ ì´ìƒ
        elif score >= 50: labels.append(1)   # ì˜ì‹¬ìŠ¤ëŸ¬ì›€
        else: labels.append(0)               # ì •ìƒ
    return labels

X = feature_engineering(df)  # ì‹œê°„, ìœ„ì¹˜, EPC íŠ¹ì§• ì¶”ì¶œ
y = create_rule_based_labels(df)  # 3í´ë˜ìŠ¤ ë¶„ë¥˜
model.fit(X, y, eval_set=[(X_test, y_test)], verbose=100)
```

### **2ë‹¨ê³„: ê·¸ë˜í”„ ì‹ ê²½ë§ (GNN)**  
**GNN ì„ íƒ ê·¼ê±°**: ê³µê¸‰ë§ ë°ì´í„°ëŠ” ë³¸ì§ˆì ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°(ìœ„ì¹˜ ê°„ ì—°ê²°ê´€ê³„)ë¥¼ ê°€ì§€ë¯€ë¡œ, ë…¸ë“œ ê°„ ê´€ê³„ ì •ë³´ë¥¼ í™œìš©í•œ ì´ìƒì¹˜ íƒì§€ê°€ ë£°ë² ì´ìŠ¤ë‚˜ ì¼ë°˜ MLë³´ë‹¤ ì •í™•ë„ê°€ ë†’ìŒ.

```python
# ê³µê¸‰ë§ ë„¤íŠ¸ì›Œí¬ë¥¼ ê·¸ë˜í”„ë¡œ ëª¨ë¸ë§
import torch_geometric
from torch_geometric.nn import GCNConv, global_mean_pool

class SupplyChainGNN(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = GCNConv(in_channels=64, out_channels=128)
        self.conv2 = GCNConv(in_channels=128, out_channels=64)
        self.classifier = torch.nn.Linear(64, 5)  # 5ê°€ì§€ ì´ìƒì¹˜ ìœ í˜•
    
# ê·¸ë˜í”„ êµ¬ì¡° ì •ì˜:
# - ë…¸ë“œ(V): 58ê°œ ìœ„ì¹˜ (ê³µì¥, ë¬¼ë¥˜ì„¼í„°, ë„ë§¤ìƒ, ì†Œë§¤ìƒ)
# - ì—£ì§€(E): EPC ì´ë™ ê²½ë¡œ (ì‹œê°„ìˆœ)
# - ë…¸ë“œ íŠ¹ì§•: [ìœ„ì¹˜_íƒ€ì…, ì¢Œí‘œ, ì²˜ë¦¬ëŸ‰]
# - ì—£ì§€ íŠ¹ì§•: [ì´ë™ì‹œê°„, EPCì •ë³´, ì´ë²¤íŠ¸íƒ€ì…]

graph_data = Data(
    x=node_features,      # [58, 64] ìœ„ì¹˜ë³„ íŠ¹ì§•
    edge_index=edge_connections,  # [2, num_edges] ì—°ê²° ì •ë³´
    edge_attr=edge_features      # [num_edges, 32] ì´ë™ íŠ¹ì§•
)
```

