### **LSTM Implementation Plan - DEFERRED**

**⚠️ PRIORITY UPDATE:** Based on assessment findings, LSTM implementation is **deferred** until existing systems are stabilized.

**Current Status:** This plan remains technically sound but is strategically premature given:
- SVM system has unresolved bugs (feature extraction, JSON serialization)
- Rule-based detection achieving only 56.6% accuracy (missing 43% of anomalies)
- Both systems need fixes before adding new complexity

**Recommended Priority Order:**
1. **Fix SVM bugs first** (highest priority) - resolve feature extraction and JSON serialization
2. **Improve rule-based detection** (medium priority) - fix jump/event order detection (currently 0%)
3. **Then consider LSTM** (future) - only after establishing solid baselines

---

### **Phase 1: Data Pipeline & Feature Engineering (Adhering to `data_requirements`)**

1.  **Data Aggregation & Enrichment:**
    *   Create a new script, `src/barcode/lstm_data_preprocessor.py`.
    *   This script will merge the raw training data (`data/raw/*.csv`).
    *   It will then enrich the merged data by joining it with the required processed files:
        *   `data/processed/location_id_withGeospatial.csv`
        *   `data/processed/business_step_transition_avg_v2.csv`
    *   The data will be sorted chronologically by `event_time` to prepare for sequencing.

2.  **Rule-Based Labeling:**
    *   Integrate the `MultiAnomalyDetector` from `src/barcode/multi_anomaly_detector.py` to generate the 5 binary ground-truth labels (`epcFake`, `epcDup`, `locErr`, `evtOrderErr`, `jump`) for each row.

3.  **Sequence Generation:**
    *   Group the labeled data by `epc_code`.
    *   Define a fixed `sequence_length` (e.g., 10, to be finalized in Q&A).
    *   Generate sequences for each EPC. Use zero-padding for sequences shorter than `sequence_length`.
    *   The target label for each sequence will be the 5-dimensional label vector of the *last* event in that sequence.

4.  **Data Splitting & Saving (tt.txt compliant):**
    *   Implement a chronological train/evaluation split (80/20) based on the timestamp of the last event in each sequence.
    *   Save the final, processed sequences and labels into a dedicated folder: `data/lstm_training/`.

---

### **Phase 2: LSTM Model Architecture (Following `ai_guidelines`)**

1.  **Model Definition (`src/barcode/lstm_detector.py`):**
    *   This new file will define the LSTM model using `torch`, leveraging the existing CUDA-enabled environment.
    *   **Architecture:**
        *   Input Layer (accepting sequences of shape `(batch_size, sequence_length, num_features)`)
        *   Two stacked `nn.LSTM` layers for robust pattern detection.
        *   `nn.Dropout` layers between LSTM layers and before the final output layer to prevent overfitting.
        *   A final `nn.Linear` layer with 5 output neurons.
        *   A `nn.Sigmoid` activation function to produce the 5 multi-label probability scores.
    *   The code will be clear and well-commented, explaining the "why" behind architectural choices.

2.  **Training Script (`train_lstm_model.py`):**
    *   This new script will orchestrate the training process.
    *   It will load the prepared data from `data/lstm_training/`.
    *   The model will be trained using `binary_crossentropy` loss and the `Adam` optimizer.
    *   The script will include GPU utilization logic (`.to(device)`).
    *   The final trained model will be saved to `models/lstm/lstm_model_YYYYMMDD.pt`.

---

### **Phase 3: API Integration (Modifying `fastapi_server.py`)**

1.  **Create a New Endpoint:**
    *   To maintain clarity and versioning, a new endpoint will be added: `POST /api/manager/export-and-analyze-async/lstm`.
    *   This keeps the production-ready rule-based endpoint and the experimental SVM endpoint separate.

2.  **Inference Logic:**
    *   The endpoint will load the trained LSTM model (`.pt` file) once at startup.
    *   For each incoming request, it will perform the necessary preprocessing and sequence construction in real-time. This is a critical step and will require careful implementation to meet the <7s performance target.
    *   The prepared sequence will be fed to the LSTM model for inference.
    *   The output will be formatted into the standard JSON response, including the `lstmScore` for each detected anomaly.

---

### **Phase 4: Evaluation & Monitoring**

1.  **Evaluation Script (`evaluate_lstm_model.py`):**
    *   This new script will be created, following the academic rigor of `evaluate_svm_models.py`.
    *   It will load the evaluation dataset from `data/lstm_training/`.
    *   It will calculate and compare the performance (Recall, Precision, F1, Specificity) of the LSTM model against the rule-based system.
    *   Results will be logged to `logs/lstm_models/lstm_performance_log.csv`.

---

### **Critical Issues to Address Before LSTM Implementation**

**Technical Challenges Identified:**

1. **Real-time Sequence Construction:** Building input sequences requires fetching last N-1 events for each EPC within <7s API requirement. No clear solution for this fundamental lookup architecture challenge.

2. **Cold Start Problem:** First events for any EPC have no historical sequence (padding-only input), requiring fallback to rule-based detection and adding system complexity.

3. **Performance vs Benefit Unclear:** Rule-based system works and is fast, SVM framework exists but needs fixes. ROI for LSTM complexity investment is uncertain.

### **Future LSTM Questions (When Ready)**

1. **Sequence Length:** Optimal `sequence_length` balancing temporal patterns vs performance (recommendation: 10-15)
2. **Caching Strategy:** Redis for recent events vs querying primary data store for real-time sequence construction  
3. **Cold Start Handling:** Low confidence scores vs rule-based fallback for first few events
4. **Architecture Complexity:** Two-layer vs single-layer LSTM performance trade-offs
5. **API Batching:** Processing events in batches vs individually for efficiency

**Next Action Required:** Complete SVM bug fixes and rule-based improvements before revisiting this plan.

---

## **10 LSTM Optimization Strategies & Research Directions**

**Context Update:** SVM bugs have been resolved (JSON serialization, field mapping, feature extraction). LSTM implementation may now be more strategically viable.

### **1. Data Leakage Prevention & Temporal Integrity**
**Issue:** Row-level LSTM training uses chronological splitting, but synthetic anomaly generation may inherit future information.
**Strategy:** Implement strict temporal data isolation:
- Generate synthetic anomalies **after** chronological split
- Tag synthetic rows with `is_synthetic` flag for separate evaluation metrics
- Use time-aware validation to prevent future information leakage
- Implement data versioning with timestamps for reproducible splits

### **2. Advanced Sequence Architecture Design**
**Issue:** Fixed sequence_length=10 may miss long-term dependencies or over-smooth short bursts.
**Strategy:** Implement adaptive sequence modeling:
- Store sequence_length as hyperparameter in `config/lstm.yaml`
- Use Bayesian optimization (Optuna) sweep over {5,10,15,20} sequence lengths
- Implement variable-length sequences with attention masking
- Consider hierarchical LSTM (event-level + EPC-level) for multi-scale patterns

### **3. Class Imbalance & Rare Event Detection**
**Issue:** Jump anomalies are rare (<5%); LSTM may learn to always predict "normal".
**Strategy:** Implement advanced imbalance handling:
- Use **focal loss** (γ = 2) during training to focus on hard examples
- Implement time-aware stratified sampling within chronological splits
- Use cost-sensitive learning with domain-specific loss weights
- Apply SMOTE-variants for temporal data (TimeGAN for sequence augmentation)

### **4. Cold Start & Missing Value Engineering**
**Issue:** First events have no historical sequence, requiring padding-only input.
**Strategy:** Implement intelligent cold start handling:
- Use sentinel value (-1) for missing previous_location_id
- Add binary flag `is_first_event` to feature vectors
- Train separate "cold start" model for first N events per EPC
- Implement warm-up period with rule-based fallback and confidence scoring

### **5. Real-time Inference Optimization**
**Issue:** Building input sequences requires fetching last N-1 events within <7s API requirement.
**Strategy:** Implement high-performance sequence caching:
- Use Redis with sliding window cache for recent EPC events
- Implement batch processing for multiple EPC predictions
- Pre-compute sequence features for frequently accessed EPCs
- Use asynchronous processing with WebSocket updates for longer sequences

### **6. Feature Drift Detection & Monitoring**
**Issue:** Location-level statistical features may drift when new factories come online.
**Strategy:** Implement production monitoring:
- Add online drift detector (Kolmogorov-Smirnov on rolling windows)
- Expose `/health/drift` endpoint returning drift scores per feature
- Implement automatic model retraining triggers based on drift thresholds
- Use domain adaptation techniques for new factory integration

### **7. Model Explainability & Trust**
**Issue:** LSTM outputs probabilities without SHAP-style explanations for regulated supply chains.
**Strategy:** Implement comprehensive explainability:
- Wrap LSTM with Integrated Gradients explainer
- Pre-compute baseline sequences (all features = 0) for attribution
- Expose `/explain/{eventId}` endpoint with per-feature attribution scores
- Implement attention visualization for sequence-level interpretability

### **8. Evaluation Metrics & Business Alignment**
**Issue:** Global recall > 0.85 target may not align with business costs (false positives blocking shipments).
**Strategy:** Implement business-aware evaluation:
- Define cost-weighted Fβ-scores (β = 0.5 for locErr, β = 2 for jump)
- Store cost weights in `config/cost_matrix.yaml` for transparent tuning
- Implement precision-recall curves for each anomaly type
- Add business impact metrics (shipment delays, false positive costs)

### **9. Model Versioning & Experiment Tracking**
**Issue:** Rolling retraining overwrites models with no traceability of data snapshots.
**Strategy:** Implement comprehensive MLOps:
- Adopt DVC (Data Version Control) for data and model artifacts
- Tag each model with Git commit hash of training snapshot
- Implement A/B testing framework for model comparison
- Use MLflow for experiment tracking and model registry

### **10. Robust Testing & Edge Case Coverage**
**Issue:** Synthetic data doesn't cover extreme cases like 1-minute jumps or 30-day gaps.
**Strategy:** Implement comprehensive testing framework:
- Create property-based test suite (Hypothesis) for controlled temporal anomalies
- Generate edge cases: ultra-fast jumps, long gaps, location loops
- Assert recall ≥ 0.9 on extreme samples across all anomaly types
- Implement stress testing with concurrent API requests and memory constraints

---

## **Implementation Priority (Post-SVM Recovery)**

**Phase 1 (Immediate):** Strategies 1, 4, 5 - Data integrity and real-time performance
**Phase 2 (Short-term):** Strategies 2, 3, 8 - Model architecture and business alignment  
**Phase 3 (Long-term):** Strategies 6, 7, 9, 10 - Production monitoring and MLOps

**Success Metrics:**
- LSTM accuracy > current rule-based baseline (56.6%)
- API response time < 7s for 50 events
- False positive rate < 5% for critical anomaly types
- Model explainability scores > 0.8 for regulatory compliance