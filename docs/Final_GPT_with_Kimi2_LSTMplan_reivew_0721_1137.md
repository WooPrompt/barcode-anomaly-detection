# ğŸ§­ Guideline for Writing a Unified and Defensible LSTM Anomaly Detection Plan


> ğŸ§‘â€ğŸ« *"You are no longer writing a plan. You are writing your defense."*

---

## 1. ğŸ“ Dataset & Labeling: Transparency Over Convenience

**âœ… Must Include:**

* Description of **real vs. simulated data** distributions (not just quantity).
* **Explicit risk** of using rule-based labels: include discussion of circularity (e.g., â€œlabels are from the same rules we later evaluate againstâ€).
* **Remedy**: mention `human audit` of 1% stratified samples or `label noise injection` test.

**ğŸ“Œ Refer to:**

* GPTâ€™s concern with label quality.
* Kimi2â€™s quote: *â€œsynthetic completeness is not real-world completeness.â€*

**ğŸ›  Actionable Inclusion:**

```markdown
We conducted a 5% label-flip experiment and observed a 6.3% AUC degradation, indicating moderate label fragility.
```

---

## 2. ğŸ›  Feature Engineering: Rigor Before Quantity

**âœ… Must Include:**

* VIF (Variance Inflation Factor) or mRMR (Minimum Redundancy Max Relevance) analysis to **prune features** (donâ€™t keep 3 variations of `time_gap_*`).
* Fix entropy instability for short sequences: **Bayesian entropy** or smoothed estimator.
* Discuss the *ordinal* nature of business steps and consider **graph-based encoding** (e.g., Hasse DAG).

**ğŸ“Œ Refer to:**

* Kimi2â€™s warning: *â€œ60 features â‰  60 degrees of freedomâ€*
* Claude's redundant inclusion of z-score, log, raw version of same features

**ğŸ›  Actionable Inclusion:**

```markdown
After VIF pruning, we retained 22 out of 61 features. This improved training stability and interpretability.
```

---

## 3. ğŸ”€ Sequence Splitting: EPC-Aware Integrity

**âœ… Must Include:**

* **EPC-level split**: ensure the same `epc_code` does not appear in both train and test, even if timestamps are 7 days apart.
* Explain why **chronological buffer** isnâ€™t sufficient to avoid leakage of long-range patterns.

**ğŸ“Œ Refer to:**

* Kimi2â€™s buffer-leak counterexample
* Claudeâ€™s current design violates this by split-by-timestamp only

**ğŸ›  Actionable Inclusion:**

```markdown
We removed EPCs whose earliest and latest events spanned the split boundary, eliminating 2.1% of records to ensure leakage-free splits.
```

---

## 4. ğŸ§  Model Architecture: Choose Justifiably

**âœ… Must Include:**

* If GRU is used: show that **gradient norm decay is acceptable** beyond 15 time steps.
* If LSTM+Attention is used: explain how many time steps justify **multi-head attention** (avoid rank-deficiency).
* If quantization is used: report **AUC drop** under symmetric vs affine quantization.

**ğŸ“Œ Refer to:**

* Claudeâ€™s strong architecture, but missing evidence for attention effectiveness
* Kimi2â€™s reminder: *â€œparameter count is second-order; gradient path length is first-orderâ€*

**ğŸ›  Actionable Inclusion:**

```markdown
We compared GRU vs. LSTM on 20-step sequences. LSTM achieved 3% better F1 and higher gradient stability at depth.
```

---

## 5. ğŸ“‰ Dimensionality Reduction: Methodologically Sound

**âœ… Must Include:**

* Reject PCA unless supported by **variance explanation + model performance gain**.
* Never rely solely on **t-SNE visualizations** to justify PCA.
* Optional: propose **hierarchical feature subnetworks** only if PCA is not used.

**ğŸ“Œ Refer to:**

* Geminiâ€™s simplicity wins, but is too reliant on t-SNE plots
* Claude justifies PCA via 5 metricsâ€”solid, but needs VIF/mRMR

**ğŸ›  Actionable Inclusion:**

```markdown
PCA was rejected as the first 20 components explained only 72% of variance, and performance dropped on a 5-fold test.
```

---

## 6. ğŸ§ª Evaluation Framework: Cost, Drift, Power

**âœ… Must Include:**

* **Cost-sensitive evaluation**: use business-weighted confusion matrix, or report **Area under Cost Curve (AUCC)**.
* **Drift detection** must report **minimum detectable effect size** or power threshold (e.g., N required for 0.05 AUC drop at 80% power).
* If SHAP is used: acknowledge **bias due to feature autocorrelation**; offer Integrated Gradients alternative.

**ğŸ“Œ Refer to:**

* Kimi2â€™s call for *AUCC and Integrated Gradients*
* Claudeâ€™s excellent evaluation checklist, but missing power audit

**ğŸ›  Actionable Inclusion:**

```markdown
We computed AUCC across a 0.01â€“100 penalty range and found the optimal operating point at cost=12.3.
```

---

## 7. ğŸŒ Real-Time Deployment: Robustness and Trust

**âœ… Must Include:**

* Cold start fallback should **not reuse the labeling rule logic**, or else itâ€™s **perfect overfitting**.
* Real-time pipeline must include **concept drift plan** and retraining trigger.
* **Explain model versioning and rollback** if inference performance drops.

**ğŸ“Œ Refer to:**

* Claudeâ€™s transfer-learning cold start logic is novel but fragile
* Kimi2â€™s brutal truth: *â€œfallback is literally the training labelsâ€*

**ğŸ›  Actionable Inclusion:**

```markdown
Fallback cold-start model uses nearest-neighbor from pre-trained vector cache, *not* rule-based logic, preventing label echo.
```

---

## 8. ğŸ§¬ Reproducibility: Academic Rigor

**âœ… Must Include:**

* Mention of random seed, RNG version (PyTorch, NumPy), and CUDA driver.
* Hardware config, GPU/CPU, RAM.
* Bonus: include **carbon/energy usage**, FLOP count or inference energy budget (aligns with Green AI principles).

**ğŸ“Œ Refer to:**

* Kimi2â€™s reproducibility checklist

**ğŸ›  Actionable Inclusion:**

```markdown
All results are reproducible with `conda-lock.yml`. CUDA 12.3, cuDNN 8.7, PyTorch 2.1.1, RTX 3090 24GB. Training used 1.3 kWh.
```

---

## ğŸ§¾ Final Output Recommendation Structure

| Section              | Title                               |
| -------------------- | ----------------------------------- |
| ğŸ“‹ Executive Summary | 1-page for stakeholders             |
| ğŸ“‚ Data & Labeling   | With limitations                    |
| ğŸ›  Preprocessing     | Feature pruning, EPC-split          |
| ğŸ§  Model Design      | Architecture with justification     |
| ğŸ“‰ Dimensionality    | Reduction policy and rationale      |
| ğŸ§ª Evaluation        | Metrics, drift, AUCC                |
| ğŸ“¡ Deployment        | Real-time fallback + API            |
| ğŸ” Reproducibility   | Hardware, seed, energy              |
| ğŸ“Š Appendix          | Ablations, experiments, power tests |

---

## ğŸ“ Closing Encouragement

Your Claude and Gemini drafts show **engineering brilliance**, but thesis defense is about **epistemic defensibility**. Follow this guide to:

* Argue what you did.
* Prove why you did it.
* Anticipate how it could fail.
* Defend it with data.

---

## ğŸ§  Summary Decision:

> **Start from Claude, but revise with Geminiâ€™s structure and Kimi2â€™s academic rigor.**

---

## ğŸ” Why?

| Criteria                       | Gemini                                                       | Claude                                                                            | What to Do                                                                                   |
| ------------------------------ | ------------------------------------------------------------ | --------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------- |
| âœ… **Architecture Depth**       | Light (GRU-based, fast, simple)                              | Deep (LSTM + Attention + Adaptive sequences + Quantized + Cold Start + Real-time) | âœ… **Start from Claude**: Itâ€™s future-proof and production-ready.                             |
| âœ… **Evaluation Framework**     | Simple (KernelSHAP, F1)                                      | Academic-level (cost-weighted F1, sequence stability, SHAP + attention weights)   | âœ… **Keep Claudeâ€™s** rich metrics & drift handling.                                           |
| âŒ **Statistical Rigor**        | Weak (no VIF, no cost analysis, no split leakage audit)      | Also weak, **but fixable**                                                        | âœ… **Use Kimi2â€™s critique** to harden Claude.                                                 |
| âœ… **Structure & Clarity**      | Excellent dual-layer explanation (technical + non-technical) | Dense and technical                                                               | âœ… **Steal Geminiâ€™s writing style**, especially in `Executive Summary` and `Data & Labeling`. |
| âŒ **EPC Split / Leakage**      | Uses time-split only                                         | Same flaw                                                                         | âœ… MUST fix Claudeâ€™s data split with **EPC-based partitioning**, as Kimi2 recommends.         |
| âŒ **Feature Redundancy Logic** | t-SNE based â€” weak                                           | Same                                                                              | âœ… Add VIF + mRMR justification and remove over-correlated features.                          |
| âŒ **Interpretability Math**    | KernelSHAP only                                              | SHAP + attention â€” better, but still biased                                       | âœ… Replace SHAP with **Integrated Gradients** or **Markov-aware SHAP** if you can.            |

---

## ğŸ› ï¸ Concrete Plan

| Task                 | Action                                                                                                                       |
| -------------------- | ---------------------------------------------------------------------------------------------------------------------------- |
| âœ… Base Document      | Use **Claudeâ€™s Plan** (itâ€™s already more complete)                                                                           |
| âœï¸ Rewrite Sections  | Replace `Data Split`, `Feature Selection`, and `Fallback` sections with corrected logic from Kimi2                           |
| âœ‚ï¸ Prune/Refactor    | Remove redundant features (e.g., keep only `time_gap_log` not raw/zscore/log combo)                                          |
| ğŸ“ˆ Add Experiments   | - AUC drop under label noise<br> - AUCC (Area Under Cost Curve)<br> - Cold-start without rules<br> - Drift power analysis    |
| ğŸ“‹ Polish Format     | Use **Geminiâ€™s stakeholder-friendly tables and dual-layer language**                                                         |
| ğŸ§  Add Justification | - Why 15 steps?<br> - Why use Attention?<br> - Why not just PCA?<br> Each decision needs both empirical & theoretical backup |
| ğŸ“ Add Appendix      | - Simulation limitations<br> - FLOP & energy budget<br> - Docker + `conda-lock.yml` for reproducibility                      |

---

## ğŸ§¾ Final Verdict

| Strategy                                      | Result                                       |
| --------------------------------------------- | -------------------------------------------- |
| Start with Gemini                             | âœ… Quick win, but shallow for defense         |
| Start with Claude                             | ğŸŸ¨ Deep, but fragile under academic critique |
| **Start with Claude + Fix with Gemini+Kimi2** | âœ…âœ… Gold-standard, professor-level thesis     |

---
