## knowns
- The project detects 5 anomaly types: epcFake, epcDup, locErr, evtOrderErr, jump.
- Backend wants anomaly info in a specific JSON format.
- I want to measure runtime per rule.
- Final output will inform frontendâ€™s timeout/loading design.
- The data is a pandas DataFrame with event history and EPCs.

## unknowns
- **Do all anomaly functions operate independently or are there dependencies?**
  - `detect_epc_fake` and `detect_epc_dup` operate independently. `detect_loc_err` depends on the data being sorted by `epc_code` and `event_time`, but is otherwise independent.
- **Which column combinations best identify each anomaly type?**
  - `detect_epc_fake`: `epc_code`
  - `detect_epc_dup`: `epc_code`, `event_time`, `scan_location`
  - `detect_loc_err`: `epc_code`, `event_time`, `scan_location` (and the derived `location_level`)
- **How much variation in runtime exists between small vs large input sizes?**
  - On a dataset of 920,000 records:
    - `detect_epc_fake` took ~8.6 seconds.
    - `detect_epc_dup` took ~77.1 seconds.
    - `detect_loc_err` took ~1.1 seconds.
- **What is the expected threshold for "too slow"?**
  - `detect_epc_dup` at 77 seconds is likely too slow for a real-time API. This will need to be optimized.
- **Are edge cases (e.g. duplicate EPCs with same timestamp) already covered in test data?**
  - The benchmark data has not triggered any anomalies so far, indicating that the test data may not contain these edge cases.

## questions_for_better_prompting

### Project & Goal Context
1.  What is the ultimate business goal of this project?
2.  Who are the end-users of this application?
3.  What is the most critical metric for success (e.g., accuracy, performance, cost)?
4.  What is the overall technical architecture of the system?
5.  Are there any existing design documents, diagrams, or specifications I should be aware of?

### Data & Schema
6.  Where does the data come from, and how is it ingested?
7.  What are the expected data volume and velocity (e.g., rows per day, streaming vs. batch)?
8.  Are there any known data quality issues or inconsistencies?
9.  Can you provide a data dictionary or detailed explanation of each column?
10. Are there any implicit relationships between columns that aren't obvious from the schema?

### Technical & Implementation Constraints
11. Are there any specific libraries, frameworks, or algorithms that you prefer or must be used?
12. Are there any performance constraints (e.g., max response time, memory usage)?
13. What is the target deployment environment (e.g., cloud, on-premise, specific OS)?
14. Are there any coding style guides or linting configurations I must adhere to?
15. What is the project's testing strategy and which testing libraries are in use?

### Task-Specific Details
16. What is the precise definition of the inputs and outputs for this specific task?
17. Can you provide a few concrete examples of inputs and their expected outputs?
18. How should the code handle errors and edge cases?
19. What is the expected format for logging and monitoring?
20. Is this a new feature, a refactoring, or a bug fix?

### AI Interaction & Learning
21. Should I prioritize code brevity, performance, or readability?
22. Do you want me to explain the 'why' behind my code, or just provide the solution?
23. Would you like me to propose alternative solutions or just the one I think is best?
24. How much context should I assume you already have?
25. Should I ask for clarification on ambiguous requests, or make a reasonable assumption?

### Broader Context & Future Plans
26. What is the next logical step after this task is complete?
27. How does this task fit into the larger project roadmap?
28. Are there any future plans to scale or modify this feature?
29. Are there any other teams or developers who will be working with this code?
30. What is the most important thing for me to get right in this task?