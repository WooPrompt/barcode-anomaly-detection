## Row-Level Multi-Label SVM Implementation Plan

### Project Overview
Convert current EPC-groupby SVM approach to row-level multi-label binary classification system.
Goal: 5 separate binary SVMs outputting probability scores for each anomaly type per event row.

### Key Requirements Summary
- Row-level processing instead of EPC groupby
- Multi-label binary classification (5 separate SVMs)
- Probability outputs with calibration
- 7-second API response time
- Focus on specificity metrics
- Train/eval data split compliance with tt.txt

---

## Questions & Answers Summary

### Initial 10 Questions:

**Q1: Row-level features - raw event data or derived features?**
A1: Use derived CSV data in data/processed folder:
- location_id_withGeospatial.csv (latitude/longitude)
- business_step_transition_avg_v2.csv (time differences between locations)
- location_id_scan_location_matching.csv (location_id and scan_location matching)

**Q2: Label generation strategy - row-level or EPC-level rule detection?**
A2: Current rule detection is already row-level (need to verify this)

**Q3: Multi-label vs Multi-class - multiple anomaly types simultaneously?**
A3: Yes, have multiple anomaly types simultaneously (e.g., both epcFake=1 AND locErr=1)

**Q4: Training data balance - only normal samples or both normal and anomalous?**
A4: Not normal only. Pipeline: load whole data → preprocessing → labeling for each anomaly → separate into train/eval → train 5 multi-label binary SVM models

**Q5: Feature extraction scope - context from surrounding events?**
A5: Need to add previous location information. Sort events within each EPC, then calculate surrounding event info and add to each row. This is needed for labeling.

**Q6: SVM architecture - 5 separate binary SVMs or one multi-output?**
A6: 5 separate binary SVMs with multi-label probability output

**Q7: Probability calibration - raw scores or calibrated probabilities?**
A7: Want properly calibrated probabilities (check existing code)

**Q8: Data preprocessing - existing pipeline or simpler row-level?**
A8: Modify existing one

**Q9: Evaluation metrics - which metrics most important?**
A9: Everything is positive. Want to know how well model catches negatives as negatives. Specificity measures work like that.

**Q10: File structure for train.csv and eval.csv?**
A10: All original + 5 binary label columns + derived labels (e.g., transition time, etc.)

### Follow-up 10 Questions:

**Q1: Current rule detection verification?**
A1: Yes, check existing rule detection code to confirm it's row-level

**Q2: Processed data integration - joined or lookup tables?**
A2: Joined with the main data

**Q3: Previous location context - how many previous events?**
A3: Immediate previous event location_id. Since scan_location is string, ML doesn't work on that. Refer to location_id_scan_location_matching.csv for matching.

**Q4: Transition time calculation - from CSV averages or dynamic?**
A4: This CSV file already contains the information

**Q5: Data pipeline sequence - replace current or run alongside?**
A5: Make new code for that

**Q6: Specificity focus - optimize for high specificity over sensitivity?**
A6: Focus on specificity. System is anomaly detection for logistic flow.

**Q7: Evaluation threshold setting - per-anomaly ROC or overall F1?**
A7: Each anomaly has different features. Use per-anomaly ROC analysis.

**Q8: Feature engineering scope - statistical features for EPC context?**
A8: Yes. Statistical features explanation:
- Temporal: epc_total_duration, epc_event_count, epc_avg_step_time, epc_std_step_time
- Spatial: epc_unique_locations, epc_location_revisits, epc_max_distance  
- Positional: event_position_in_sequence, events_remaining, progress_ratio

**Q9: Memory/performance considerations - batch size/processing time?**
A9: Optimize it. When BE calls API, model must respond with JSON format like rule-based does, within 7 seconds.

**Q10: Backwards compatibility - coexist or replace current system?**
A10: Check fastapi_server.py - using both rule-based and model-based API

---

## Implementation Plan

### Phase 1: Data Pipeline Creation
1. **Create new row-level data processor** (`row_level_data_processor.py`)
   - Load all CSV files from `data/raw/`
   - Join with processed data: geospatial, transition times, location matching
   - Add previous location context for each row
   - Generate statistical features per EPC sequence
   - Apply rule-based labeling to create 5 binary labels per row

2. **Generate train.csv and eval.csv**
   - Structure: `[original_columns + derived_features + epcFake_label + epcDup_label + locErr_label + evtOrderErr_label + jump_label]`
   - 80/20 split maintaining tt.txt compliance
   - Save in `data/svm_training/` directory

### Phase 2: SVM Architecture Update
1. **Modify SVM detector** (`svm_anomaly_detector.py`)
   - Convert from unsupervised to 5 separate binary SVMs
   - Implement probability calibration (Platt scaling)
   - Add multi-label prediction with 5 probability outputs
   - Optimize for specificity over sensitivity

2. **Update training pipeline** (`train_svm_models.py`)
   - Use train.csv for supervised learning
   - Train 5 binary SVMs independently
   - Per-anomaly ROC analysis for threshold optimization

### Phase 3: API Integration
1. **Update FastAPI endpoints**
   - Modify existing SVM endpoint to return 5 probability scores
   - Maintain 7-second response time requirement
   - Keep rule-based API as fallback
   - Ensure JSON format compatibility

### Phase 4: Evaluation System
1. **Update evaluation pipeline**
   - Use eval.csv for testing
   - Focus on specificity metrics
   - Per-anomaly ROC curves
   - Generate comprehensive performance reports

### Technical Details

**Statistical Features to Add:**
- `epc_total_duration`: Total time from first to last event in EPC
- `epc_event_count`: Number of events in this EPC sequence
- `epc_avg_step_time`: Average time between consecutive events
- `epc_std_step_time`: Standard deviation of step times (irregularity indicator)
- `epc_unique_locations`: Number of unique locations visited
- `epc_location_revisits`: Count of location revisits (possible loops)
- `epc_max_distance`: Maximum geographical distance between consecutive locations
- `event_position_in_sequence`: This event's position (1st, 2nd, 3rd, etc.)
- `events_remaining`: How many events left in this EPC sequence
- `progress_ratio`: event_position / total_events (0.0 to 1.0)
- `previous_location_id`: Location ID of immediate previous event

**Data Files to Integrate:**
- `data/processed/location_id_withGeospatial.csv`
- `data/processed/business_step_transition_avg_v2.csv`
- `data/processed/location_id_scan_location_matching.csv`

**Performance Requirements:**
- API response time: < 7 seconds
- Focus on specificity (true negative rate)
- Per-anomaly ROC analysis for threshold optimization
- Maintain compatibility with existing rule-based API

This plan replaces the current EPC-groupby approach with row-level processing while maintaining API compatibility and improving anomaly detection capabilities.