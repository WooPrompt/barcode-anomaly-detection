## question_loop (ACTIVE)
This questioning protocol MUST be applied in full to the current task.


## goal
Break down a rule-based anomaly detection script into five individual anomaly-checking functions, test each separately, and analyze runtime + anomaly count for each to inform frontend API timeout settings.

## context
Project uses FastAPI + rule-based & ML-based detection. You're testing `epcFake`, `epcDup`, `locErr`, `evtOrderErr`, `jump` detection rules, and want to isolate/analyze them.

### When responding, follow this checklist:

1. Summarize the user's **motivation and goal** for this request.
2. Identify any **ambiguities** or unclear parts between related concepts in the instruction (e.g., between A and B).
3. Clearly state your own **interpretation** of the request using a numbered list (1, 2, 3...).
4. Suggest at least **3 possible solution strategies**, with short pros and cons.
5. If similar ideas or concepts have appeared before, **compare and contrast** them.
6. Point out **edge cases** or exception scenarios and explain how your solution would handle them.
7. Visualize logic clearly with:
   - Minimal example code snippets
   - Text-based flow diagrams if possible
   - Table structure if working with DataFrames
8. Summarize the current understanding and **check the flow step-by-step**. Example: “So far, we’re doing A → B → C”.
9. Offer at least **one way this logic could be reused or applied** in a different project or context.
10. Suggest **3 ways the user could ask a better question next time**, based on this interaction.


## instructions
1. Generate code to isolate and test each anomaly rule as a separate function.
2. Save runtime + result count per rule.
3. Output summary of which rule is:
   - Most frequently triggered
   - Takes longest to run

## knowns
- The project detects 5 anomaly types: epcFake, epcDup, locErr, evtOrderErr, jump.
- Backend wants anomaly info in a specific JSON format.
- I want to measure runtime per rule.
- Final output will inform frontend’s timeout/loading design.
- The data is a pandas DataFrame with event history and EPCs.

## unknowns
- Do all anomaly functions operate independently or are there dependencies?
- Which column combinations best identify each anomaly type?
- How much variation in runtime exists between small vs large input sizes?
- What is the expected threshold for "too slow"?
- Are edge cases (e.g. duplicate EPCs with same timestamp) already covered in test data?

## output_format
Save a file called `question.txt` that includes both the `## knowns` and `## unknowns` sections above.
After code runs, prompt me to update `question.txt` by filling in answers under `## unknowns`.

## user_instruction
After I run the code, read the `question.txt` file and fill in answers to the unknowns.
Then update your internal context and replan the next step.
Repeat until all unknowns are resolved or flagged.

