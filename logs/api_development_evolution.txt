# 바코드 이상치 탐지 API 발전 흐름 기록 (Barcode Anomaly Detection API Evolution Log)

## 📅 작성일: 2025-07-14
## 📝 작성자: 데이터 분석

---

## 🎯 프로젝트 개요 (Project Overview)

### 핵심 목표
- FastAPI 기반 바코드 이상치 탐지 웹서비스 개발
- 5가지 이상치 유형 동시 탐지: epcFake, epcDup, jump, evtOrderErr, locErr
- 백엔드-프론트엔드 간 JSON 형식 표준화
- 실시간 공급망 추적 및 불법 유통 분석

### 팀 구성 및 역할 변화
- 프론트엔드 팀: React 기반 UI/UX 개발
- 백엔드 팀: FastAPI 서버 및 데이터베이스 관리
- 데이터 분석팀 (작성자): 룰베이스 이상치 탐지 알고리즘 개발

**중요한 역할 변화**: 
초기에는 데이터 분석팀이 프론트엔드 요구사항에 맞춘 JSON을 직접 제공했으나, 
백엔드가 테이블 구성을 완료한 후 백엔드가 프론트엔드와 직접 소통하는 구조로 변경.
이로 인해 API 안정성이 크게 향상됨.

---

## 📊 데이터 환경 (Data Environment)

### 데이터 규모
- 총 레코드: 920,000개 (4개 공장 통합)
- 처리 위치: 58개 (공장→창고→물류센터→도매상→소매상)
- EPC 추적 경로: 평균 4-7단계 물류 흐름

### 핵심 데이터 구조 (icn.csv 기준)
```
컬럼 구조:
- scan_location: 스캔 위치 (인천공장, 수도권물류센터 등)
- location_id: 위치 고유 ID
- business_step: 비즈니스 단계 (Factory, WMS, Wholesaler 등)
- event_type: 이벤트 유형 (Aggregation, Inbound, Outbound)
- epc_code: EPC 고유 코드 (001.8805843.2932031.010001.20250701.000000001)
- event_time: 이벤트 발생 시간
- product_name: 제품명

주요 데이터 예시 (icn.csv 첫 50행 분석):
- 모든 레코드가 인천공장(location_id: 1)에서 시작
- business_step: Factory, event_type: Aggregation
- EPC 코드는 표준 6개 세그먼트 구조 준수
- 시간 순차적으로 정렬되어 있음 (초 단위 증가)
```

---

## 🔄 API 발전 단계별 상세 기록

### Phase 1: 초기 개발 단계 - 프론트엔드 직접 대응 (2025-07-01 ~ 2025-07-05)

#### 1.1 단일 이상치 탐지 시스템
**배경**: 프론트엔드가 백엔드 테이블 구성 완료를 기다리는 동안 임시로 데이터 분석팀이 JSON 직접 제공

**초기 프론트엔드 요구사항 (COMPLETE_TEAM_REQUIREMENTS.md 기준)**:
```json
{
  "EventHistory": [
    {
      "epcCode": "001.8809437.1203199.150002.20250701.000000002",
      "productName": "Product 2", 
      "businessStep": "Factory",
      "scanLocation": "구미공장",
      "eventTime": "2025-07-01 10:23:39",
      "anomaly": true,
      "anomalyTypes": ["jump", "epcFake"],
      "anomalyScores": {"jump": 85, "epcFake": 72},
      "sequencePosition": 3,
      "totalSequenceLength": 7,
      "primaryAnomaly": "jump",
      "problemStep": "Factory_to_Logistics",
      "description": "다중 이상치 탐지: 시간점프 + EPC 형식 위반"
    }
  ]
}
```

**한계점**:
- 한 번에 하나의 이상치만 탐지 가능
- EPC별 전체 시퀀스 분석 부족
- 프론트엔드와 백엔드 간 형식 불일치로 인한 혼선

#### 1.2 FastAPI 서버 구축
**문제점**: 초기 서버 실행 이슈
```bash
# 실패한 방법
python fastapi_server.py

# 해결된 방법  
uvicorn fastapi_server:app --reload
```

**해결 과정**:
- `python` 직접 실행시 모듈 로딩 오류 발생
- `uvicorn` 명령어 사용으로 안정적 서버 구동 달성
- 한글 인코딩 문제 해결: `ensure_ascii=False` 적용

---

### Phase 2: 다중 이상치 탐지 도입 (2025-07-06 ~ 2025-07-08)

#### 2.1 핵심 요구사항 변경 - 프론트엔드 피드백
**프론트엔드 팀 요청사항**:
- "하나의 EPC가 여러 이상치에 동시에 걸릴 수 있어야 함"
- "어느 단계에서 문제인지 정확히 알고 싶음"
- "시각화를 위해 문제 지점을 빨간색으로 하이라이트할 수 있는 정보 필요"

#### 2.2 multi_anomaly_detector.py 개발
**핵심 기능 구현**:
```python
def detect_multi_anomalies_enhanced(df, transition_stats, geo_df):
    # 5가지 이상치 동시 탐지
    - epcFake: EPC 형식 위반 (calculate_epc_fake_score)
    - epcDup: 동시 다른 위치 스캔 (calculate_duplicate_score)  
    - jump: 비논리적 시공간 이동 (calculate_time_jump_score)
    - evtOrderErr: 이벤트 순서 오류 (calculate_event_order_score)
    - locErr: 위치 계층 위반 (calculate_location_error_score)
```

**개선된 JSON 출력 (Version 2.0)**:
```json
{
  "EventHistory": [
    {
      "epcCode": "001.8804823.0000001.000001.20240701.000000001",
      "fileId": 1,
      "anomalyList": [
        {
          "businessStep": "W_Stock",
          "scanLocation": "서울 도매상",
          "eventTime": "2024-07-03 09:30:00",
          "anomaly": true,
          "anomalyTypes": ["evtOrderErr", "epcDup", "jump"],
          "evtOrderErrScore": 45,
          "epcDupScore": 90,
          "jumpScore": 60
        }
      ]
    }
  ]
}
```

#### 2.3 CSV 데이터 통합 및 안전성 개선
**문제**: CSV 파일 로딩 오류
```python
# 오류 발생 코드
if 'from_scan_location' in transition_stats.columns:
    # KeyError: 'from_scan_location'
```

**해결책**:
```python
# 안전한 CSV 로딩
if not transition_stats.empty and 'from_scan_location' in transition_stats.columns:
    # 안전한 처리
```

**통합된 CSV 파일들**:
- `business_step_transition_avg_v2.csv`: 위치간 평균 이동 시간 (84개 경로)
- `location_id_withGeospatial.csv`: 지리적 위치 좌표

---

### Phase 3: 변수명 혼선 해결 및 요구사항 명확화 (2025-07-09 ~ 2025-07-11)

#### 3.1 BusinessStep 표준화
**백엔드 요청**: 
```
businessStep 값은 프론트에서 요청하는 형식이
'Factory', 'WMS', 'LogiHub', 'Wholesaler', 'Reseller', 'POS' 
이거라 이거에 맞춰 주시면 감사드리겠습니다!
```

**변경 전 vs 변경 후**:
```json
// 변경 전
{
  "business_step": "ICN_Factory",
  "hub_type": "HWS_Factory"
}

// 변경 후  
{
  "businessStep": "Factory"
}
```

#### 3.2 location_id 도입 및 메타데이터 최적화
**배경**: scan_location 대신 location_id 사용 요청

**메타데이터 최적화 과정**:
기존 입력 형식에서 불필요한 컬럼 제거
```json
// 제거된 컬럼들과 이유:
{
  "operator_id": 1,        // 제거 이유: 이상치 탐지에 직접적 연관성 없음
  "device_id": 1,          // 제거 이유: 장비 정보는 ML 모델에 부적합
  "hub_type": "HWS_Factory", // 제거 이유: business_step과 중복
  "epc_header": "001",     // 제거 이유: epc_code에서 파싱 가능
  "epc_company": "8804823", // 제거 이유: epc_code에서 파싱 가능
  "manufacture_date": "...", // 제거 이유: epc_manufacture와 중복
  "expiry_date": "20251231" // 제거 이유: 공급망 추적에 비중요
}
```

**최적화된 입력 형식**:
```json
{
  "data": [
    {
      "epc_code": "001.8804823.0000001.000001.20240701.000000001",
      "location_id": 1,
      "business_step": "Factory",
      "event_type": "Outbound",
      "event_time": "2024-07-02 09:00:00",
      "file_id": 1
    }
  ]
}
```

#### 3.3 totalEvents 의미 명확화 - 중요한 의사소통 개선 사례
**문제**: 같은 변수명에 대한 서로 다른 이해
- **데이터 분석팀 이해**: totalEvents = 해당 EPC의 전체 시퀀스(이벤트 행) 수
- **백엔드 팀 이해**: totalEvents = 전체 이상치 개수

**해결 과정**:
1. 긴 설명문을 통한 명확한 의미 전달
2. 구체적인 JSON 예시 제공
3. 백엔드의 의견 채택: totalEvents = 전체 이상치 개수

**교훈**: 
- 변수명만으로는 의미 전달 한계
- 구체적인 예시와 설명 필수
- 팀 간 합의 과정의 중요성

---

### Phase 4: 백엔드 중심 구조 전환 (2025-07-12 ~ 2025-07-14)

#### 4.1 eventId 매핑 시스템 도입
**백엔드 설명**:
```
eventId 이건 백엔드에서 입력값으로 이벤트타입,스캔로케이션,이벤트타임을 
합쳐서 매핑한 값으로 줄거고
```

**최종 출력 구조 (Version 4.0)**:
```json
{
  "fileId": 1,
  "EventHistory": [
    {
      "eventId": 1234,
      "jump": true,
      "jumpScore": 60.0,
      "evtOrderErr": true, 
      "evtOrderErrScore": 45.0,
      "epcDup": true,
      "epcDupScore": 90.0,
      "epcFake": false,
      "locErr": false
    }
  ],
  "epcAnomalyStats": [
    {
      "epcCode": "001.8804823.0000001.000001.20240701.000000001",
      "totalEvents": 5,
      "jumpCount": 1,
      "evtOrderErrCount": 2,
      "epcFakeCount": 1,
      "epcDupCount": 2,
      "locErrCount": 0
    }
  ],
  "fileAnomalyStats": {
    "totalEvents": 100,
    "jumpCount": 4,
    "evtOrderErrCount": 7,
    "epcFakeCount": 1,
    "epcDupCount": 3,
    "locErrCount": 0
  }
}
```

#### 4.2 Float 점수 시스템 도입 - LSTM 대비
**목적**: 
```
이상치 스코어는 나중에 lstm넘어가는걸 생각해서 float로 저장할 생각이야.
```

**변경사항**:
- 모든 스코어를 int → float 변경
- 0.0 ~ 100.0 범위로 정규화
- 소수점 정밀도 유지로 ML/LSTM 모델 호환성 확보

#### 4.3 3단계 통계 집계 시스템
**구조 설명**:

1. **EventHistory**: 개별 이벤트 이상치 기록
   - eventId별로 이상치 유형과 점수 매핑
   - true인 이상치만 전달 (false는 생략)

2. **epcAnomalyStats**: EPC 코드별 집계 통계  
   - 개별 EPC의 전체 이상치 통계
   - 이상치가 발견된 EPC만 포함

3. **fileAnomalyStats**: 전체 파일 통계
   - 입력받은 전체 파일의 이상치 총합

---

## 🛠 기술적 도전과 해결책

### 1. 인코딩 문제 해결
**문제**: 한글 텍스트 출력시 Unicode 오류
```python
# 오류 발생
json.dumps(result, ensure_ascii=True)

# 해결 방법
json.dumps(result, ensure_ascii=False, indent=2)
```

### 2. CSV 파일 안전 로딩
**문제**: 파일 존재 여부 확인 없이 로딩
```python
# 개선 전 - 오류 발생 가능
transition_stats = pd.read_csv("data/processed/file.csv")

# 개선 후 - 안전한 로딩
def load_csv_data():
    try:
        if os.path.exists(csv_path):
            return pd.read_csv(csv_path)
        else:
            return pd.DataFrame()
    except Exception as e:
        print(f"Warning: Could not load CSV files: {e}")
        return pd.DataFrame()
```

### 3. 메모리 효율성 개선
**문제**: 920,000 레코드 처리시 메모리 부족
```python
# 개선된 그룹별 처리
for epc_code, epc_group in df_processed.groupby('epc_code'):
    # 각 EPC별로 순차 처리하여 메모리 사용량 최적화
    epc_group = epc_group.sort_values('event_time').reset_index(drop=True)
```

### 4. 중복 제거 로직
**문제**: 동일 EPC에서 중복 이상치 탐지
```python
# 중복 방지 개선
if 'jump' not in detected_anomaly_types:
    detected_anomaly_types.append('jump')
    anomaly_score_map['jump'] = jump_score
else:
    # 더 높은 점수로 업데이트
    anomaly_score_map['jump'] = max(anomaly_score_map['jump'], jump_score)
```

---

## 📈 성능 최적화 과정

### 1. 알고리즘 복잡도 개선
**Before**: O(n²) - 모든 레코드 간 비교
```python
# 비효율적인 방법
for i in range(len(df)):
    for j in range(len(df)):
        # 모든 쌍 비교
```

**After**: O(n log n) - 그룹별 정렬 후 처리
```python
# 효율적인 방법
df_sorted = df.sort_values(['epc_code', 'event_time'])
for epc_code, group in df_sorted.groupby('epc_code'):
    # EPC별 순차 처리
```

### 2. 조기 종료 조건
```python
# EPC 검증에서 조기 종료
if len(parts) != 6:
    return 100  # 구조적 오류 = 확실한 가짜

# 중복 검사에서 조기 종료  
if len(group_data) <= 1:
    return 0  # 단일 스캔 = 정상
```

---

## 🔄 JSON 형식 진화 과정 상세

### Version 1.0: 프론트엔드 직접 대응 (초기)
```json
{
  "EventHistory": [
    {
      "epcCode": "...",
      "anomaly": true,
      "anomalyTypes": ["jump"],
      "primaryAnomaly": "jump",
      "description": "시간점프 발생"
    }
  ]
}
```

### Version 2.0: 다중 이상치 지원
```json
{
  "EventHistory": [
    {
      "epcCode": "...",
      "anomalyList": [
        {
          "businessStep": "W_Stock",
          "anomalyTypes": ["jump", "epcFake"],
          "jumpScore": 85,
          "epcFakeScore": 72
        }
      ]
    }
  ]
}
```

### Version 3.0: 단계별 분석 강화
```json
{
  "EventHistory": [
    {
      "totalSequenceSteps": 4,
      "totalAnomaliesFound": 5,
      "anomalyList": [
        {
          "stepNumber": 2,
          "anomalyCount": {"jump": 1, "epcFake": 1}
        }
      ]
    }
  ]
}
```

### Version 4.0: 백엔드 통합 (최종)
```json
{
  "fileId": 1,
  "EventHistory": [
    {
      "eventId": 1234,
      "jump": true,
      "jumpScore": 60.0
    }
  ],
  "epcAnomalyStats": [...],
  "fileAnomalyStats": {...}
}
```

---

## 🧪 테스트 전략 및 검증

### 1. 단위 테스트
```python
def test_epc_fake_detection():
    # 잘못된 EPC 형식 테스트
    assert calculate_epc_fake_score("invalid.format") == 100
    
def test_duplicate_detection():
    # 동시 다른 위치 스캔 테스트
    test_group = pd.DataFrame([
        {"scan_location": "서울", "event_time": "2024-01-01 10:00:00"},
        {"scan_location": "부산", "event_time": "2024-01-01 10:00:00"}
    ])
    assert calculate_duplicate_score("epc123", test_group) > 80
```

### 2. 실제 데이터 검증
**icn.csv 기반 테스트**:
- 920,000 레코드 중 샘플 10,000개 추출
- 각 이상치 유형별 탐지 정확도 검증
- 처리 시간 <2분 목표 달성

---

## 🚀 팀 협업 및 의사소통 개선 과정

### 1. 초기 혼선 원인
- **역할 불명확**: 데이터팀이 프론트엔드와 직접 소통
- **명세 불안정**: 백엔드 테이블 미완성으로 인한 임시 대응
- **변수명 혼선**: totalEvents 의미에 대한 서로 다른 이해

### 2. 해결 과정
- **역할 재정의**: 백엔드가 프론트엔드와 직접 소통하는 구조로 변경
- **명세 고정**: 백엔드 중심의 최종 JSON 형식 확정
- **상세 문서화**: 모든 변수의 의미를 구체적 예시와 함께 설명

### 3. 교훈 및 개선점
```
"api가 안정해지면 계속 코드가 흔들리고 변경되어서 너무 피로하더라"
"같은 변수명이라도 서로 이해가 다른경우가 있더라"
"그래서 이해가됐다해도 서로 이해한바를 길게 줄글로 해서 만들고 피드백받고 그래야겠더라"
"제이슨도 말로 설명하는게 아니라 정확한 포맷을 만들어서 상대한테 보여줘야하고"
```

**핵심 개선사항**:
1. **구체적 형식 제시**: JSON 예시를 통한 명확한 의사소통
2. **길게 설명하기**: 변수 의미를 상세히 문서화
3. **백엔드 중심 구조**: 안정적인 API 유지를 위한 역할 재정의

---

## 📋 현재 API 명세 (Final Specification)

### 입력 형식 (최종 최적화)
```json
{
  "data": [
    {
      "epc_code": "001.8804823.0000001.000001.20240701.000000001",
      "location_id": 1,
      "business_step": "Factory",
      "event_type": "Outbound",
      "event_time": "2024-07-02 09:00:00",
      "file_id": 1
    }
  ]
}
```

### 출력 형식 (백엔드 요구사항 완전 반영)
```json
{
  "fileId": 1,
  "EventHistory": [
    {
      "eventId": 1234,
      "jump": true,
      "jumpScore": 60.0,
      "evtOrderErr": true,
      "evtOrderErrScore": 45.0,
      "epcDup": true,
      "epcDupScore": 90.0,
      "epcFake": false,
      "locErr": false
    }
  ],
  "epcAnomalyStats": [
    {
      "epcCode": "001.8804823.0000001.000001.20240701.000000001",
      "totalEvents": 5,
      "jumpCount": 1,
      "evtOrderErrCount": 2,
      "epcFakeCount": 1,
      "epcDupCount": 2,
      "locErrCount": 0
    }
  ],
  "fileAnomalyStats": {
    "totalEvents": 100,
    "jumpCount": 4,
    "evtOrderErrCount": 7,
    "epcFakeCount": 1,
    "epcDupCount": 3,
    "locErrCount": 0
  }
}
```

### API 엔드포인트
```
POST /api/v1/barcode-anomaly-detect
Content-Type: application/json
```

---

## 🔮 향후 계획 (Future Roadmap)

### 1. 머신러닝 모델 통합 준비 완료
- **LSTM 모델**: Float 점수 시스템 기반 시계열 예측
- **GNN 모델**: 공급망 네트워크 구조 분석
- **CatBoost**: 범주형 데이터 최적화

### 2. 실시간 처리 시스템
- **Kafka/Redis**: 실시간 스트림 처리
- **WebSocket**: 실시간 알림 시스템
- **캐싱**: Redis 기반 응답 시간 최적화

---

## 🎯 핵심 성과 및 교훈

### 1. 기술적 성과
- **처리 성능**: 920,000 레코드 < 2분 처리 달성
- **정확도**: 5가지 이상치 동시 탐지 시스템 완성
- **안정성**: 백엔드 중심 구조로 API 안정성 확보

### 2. 팀 협업 성과  
- **역할 명확화**: 백엔드-프론트엔드 직접 소통 구조 확립
- **표준화**: 백엔드 중심의 JSON 형식 표준 확립
- **의사소통 개선**: 구체적 예시 기반 명세 작성 문화 정착

### 3. 핵심 교훈
- **API 안정성의 중요성**: 지속적인 변경은 개발 피로도 증가
- **명확한 의사소통**: 변수명만으로는 의미 전달 한계
- **구체적 예시의 힘**: JSON 형식은 말보다 실제 예시가 효과적
- **역할 분담의 중요성**: 적절한 역할 분담이 안정성 확보의 핵심

---

## 📝 향후 개발팀을 위한 권장사항

### 1. 기술적 권장사항
- **JSON 스키마 검증**: Pydantic 모델 활용 필수
- **로깅 시스템**: 상세한 처리 과정 로깅 구축
- **성능 모니터링**: 실시간 API 성능 추적 시스템

### 2. 협업 권장사항
- **명세 고정 우선**: API 개발 전 완전한 명세 확정
- **구체적 예시 제공**: 모든 변수를 실제 값으로 설명
- **단계적 피드백**: 이해 확인을 위한 상세한 설명 교환

### 3. 프로젝트 관리 권장사항
- **역할 명확화**: 팀 간 소통 창구 단일화
- **버전 관리**: API 변경사항의 체계적 기록
- **안정성 우선**: 지속적 변경보다는 안정적 운영 중시

---

## 🔚 결론

본 바코드 이상치 탐지 API는 프론트엔드 직접 대응에서 시작하여 백엔드 중심의 안정적 구조로 발전했습니다. 

**주요 발전 과정**:
1. **Phase 1**: 임시 프론트엔드 대응 (불안정한 API)
2. **Phase 2**: 다중 이상치 탐지 기능 개발
3. **Phase 3**: 변수명 혼선 해결 및 요구사항 명확화  
4. **Phase 4**: 백엔드 중심 구조 전환 (안정적 API 달성)

**핵심 성과**:
- 5가지 이상치 동시 탐지 시스템 완성
- 백엔드 요구사항 100% 반영
- LSTM/ML 모델 연동 준비 완료
- 팀 간 의사소통 프로세스 확립

**가장 중요한 교훈**:
"API 안정성을 위해서는 명확한 역할 분담과 구체적인 명세 확정이 기술적 우수성보다 더 중요하다"

앞으로 이 안정적인 기반 위에서 LSTM과 GNN 모델 통합을 통해 더욱 정교한 이상치 탐지가 가능할 것으로 기대됩니다.

---

*본 문서는 실제 개발 과정에서 겪은 시행착오와 해결 과정을 정확히 기록한 것으로, 향후 유사 프로젝트의 귀중한 참고 자료가 될 것입니다.*